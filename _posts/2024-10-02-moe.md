---
layout: post
title: "Mixture of Experts: From Foundations to Advanced Applications in LLMs"
date: 2024-10-02
description: ""
categories: llm
citation: true
toc:
  beginning: true
---

#  Introduction to Mixture of Experts (MoE): Concepts and Origins

This section is inspired by Chapter 11 of the amazing book Machine Learning: A Probabilistic Perspective by Kevin P. Murphy.

## Understanding the Problem: Direct vs. Inverse Prediction
Let's say we want to fit this distribution of y given x:

We first introduce the direct problem:
<div class="row justify-content-center" id="fig-2">
    <div class="col-sm-8 mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/mixture_of_experts/direct_problem.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Direct problem distribution
</div>

As we will see shortly after we swapped the x and y axis as the problem of interest will actually be the inverse of this one.
We can see here that predicting x given y corresponds to predicting a trend x=f(y) and accounting for some measuring noise.
As the trend is a function mapping from y to x (with only one x associated with a given y), the distribution we are trying to predict is unimodal. A sensible model to fit to this data would thus have this form:

$$y \sim \mathcal{N}(\mu(x \mid \theta),\sigma^2)$$

The parameters to fit for this model are the parameters $$\theta$$ of $$\mu(x \mid \theta)$$ estimating the mode/trend and $$\sigma^2$$ accounting for the residual noise. $$\mu(x \mid \theta)$$ could be any sort of model: linear regression, polynomial regression, kernel-based model, neural network...

This problem description is very classic and enables to simply fit by trying to minimize the NLL (Negative Log Likelihood).

However, considering the inverse problem by predicting y as a function of x does not lead to such nice properties:
<div class="row justify-content-center" id="fig-2">
    <div class="col-sm-8 mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/mixture_of_experts/inverse_problem.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Inverse problem distribution
</div>

While the direct problem led to a simple unimodal distribution, we see that the inverse problem has 2 modes for x between 0.4 and 0.6, making the previous model obsolete. We see that such a phenomenon of a multimodal inverse problem arises whenever we consider the inverse of a function that is not injective.

Luckily, Mixture of Experts (MoE) (first introduced in {% cite adaptative_mixture_of_experts%})is a type of model that enables to solve this issue. MoEs belong to the class Latent Variable Models (LVM) that rely on some hidden variables (the latent variables) to make predictions.

More formally, we could introduce a categorical latent variable z such that $$p(z\mid x)=Cat(z\mid Softmax(V^T(1,x)))$$ that will determine the "expert" to use. The function $$p(z\mid x)$$ is called the gating function. Each of the N experts can then be a model of the form described earlier $$p(y\mid x,z=k)=\mathcal{N}(y\mid\mu_k(x \mid \theta_k),\sigma_k^2)$$ describing an unimodal distribution. As for a given x we could have a nonzero probability of "activating" each expert, we can then describe any distribution having less than N modes.

The model parameters are then $$\theta_1,...,\theta_k,\sigma_1^2,...,\sigma_k^2$$ and $$V \in \mathcal{M}_{2 \times N}$$. We denote by $$\theta$$ all these parameters.

## Fitting Mixture of Experts Models: The EM Algorithm Explained

Fitting a Latent Variable Model is not straightforward. The joint log probability $$log(p(y,z\mid x,\theta))$$ is easy to compute but the observed data log likelihood is hard to compute since $$log(p(y\mid x,\theta))=log(\sum p(y,z\mid x,theta))$$.

The log can't be pushed into the sum. Moreover, most of the simple models lead to a convex log likelihood but this property is lost for LVM as the logarithm of the sum of 2 log convex functions is not necessarily convex, making the optimization problem much harder.

As the complete data log probability is easy to compute, we estimate the observed data log likelihood log(p(x)) by the expected complete data log likelihood $$\mathbb{E}_{z \sim q(z\mid x,y)}[log(P(y,z\mid x))]$$

The EM algorithm does exactly that in order by alternating between 2 steps:

- The E (Estimation) step where we estimate $$q^t(z_i\mid x_i,y_i)=p(z_i=k\mid x_i,y_i,\hat{\theta}^t)$$

- The M (Maximization) step where we compute $$\hat{\theta}^{t+1}=\underset{\theta}{argmax}\sum_{i=1}^n\mathbb{E}_{z_i \sim q^t(z_i\mid x_i,y_i)}[log(P(y_i,z_i\mid x_i))]$$

In other words, the EM algorithm iteratively computes

$$
\begin{align*}
&\hat{\theta}^{t+1}=\underset{\theta}{argmax}Q(\theta,\hat{\theta}^{t}) \\
&Q(\theta,\hat{\theta}^{t})=\sum_{i=1}^n\sum_{k=1}^Nlog[p(y_i,z_i=k\mid x_i,\theta)]p(z_i=k\mid x_i,\hat{\theta^t})
\end{align*}
$$

This method can be used whenever we can approximate the posterior probability in the E step. It is however also necessary to be able to fit rapidly the expert models (for example by having closed form formulas or a convex NLL) as we will have to fit the N experts for every iteration.

## Practical Application: Implementing MoEs with Linear Regression Experts

For this specific problem, we use 3 experts which are all linear regressions:

$$
\begin{align*}
&p(z\mid x)=Cat(z\mid Softmax(V^T(1,x))) \\
&p(y\mid x,z=k)=\mathcal{N}(y\mid \omega_k^Tx,\sigma_k^2)
\end{align*}
$$

For a given time step t, we can compute the E step using Bayes rule:

$$
r_{ik}=p(z_i=k \mid x_i,y_i)=\frac{p(y_i \mid x_i,z_i=k)p(z_i=k \mid x)}{\sum_{k=1}^3p(y_i \mid x_i,z_i=k)p(z_i=k \mid x)}
$$

For the M step, the expected complete data log likelihood is given by:

$$
\begin{align*}
Q(\theta, \theta^{old}) &= \sum_{i=1}^{N} \sum_{k=1}^{3} r_{ik}  \mathcal{N}(y_i | \mathbf{w}_k^T \mathbf{x}_i, \sigma_k^2) \\
&= \sum_{i=1}^{N}\sum_{k=1}^{3}r_{ik}\log[\mathcal{S}(\mathbf{V}^T \mathbf{x}_i)_k] + \sum_{k=1}^{3}\sum_{i=1}^{N}r_{ik}(-\frac{1}{\sigma_k^2}(y_i-w_k^Tx_i)^2)
\end{align*}
$$

We can recognize that the terms involving $$\mathbf{V}$$ correspond to -1 times the soft cross entropy loss with soft labels $$r_{ik}$$.
Hence, determining $$\mathbf{V}$$ simply corresponds to fitting a multiclass logistic regression with soft labels.

For each k, the terms involving $$\omega_k$$ and $$\sigma_k^2$$ correspond to:

$$
\sum_{i=1}^{N} r_{ik} \left\{ -\frac{1}{\sigma_k^2} (y_i - \mathbf{w}_k^T \mathbf{x}_i)^2 \right\}
$$

The objective function for each $$w_k$$ can then be recognized as a weighted least square problem which can also be optimized efficiently through linear algebra or convex optimization.

<div class="row justify-content-center" id="fig-2">
    <div class="col-sm-6 mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/mixture_of_experts/linear_regression_mixture_of_experts.png" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm-6 mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/mixture_of_experts/gating_function_mixture_of_experts.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Mixture of Experts model fitted to our inverse problem.
</div>

We can see once again the power of the EM algorithm that incrementally optimizes a highly non-convex objective while having simple steps optimizing convex objectives.

## Theoretical Foundations: Why the EM Algorithm Works

We will show in this part that the EM algorithm monotonically increases the observed data log likelihood.

Our first goal is to derive a formula showing how far our estimation of the observed data log likelihood by the expected complete data log likelihood is.

To make the point more general, we assume the variable z is continuous (the same results also hold for discrete z)

$$
\begin{align*}
L_{\theta,q}(x,y)=\mathbb{E}_{z \sim q(z\mid x,y)}[\log(p(y,z\mid x,\theta))] &= \int_z q(z\mid x,y) \log(p(y,z\mid x,\theta)) \, dz \\
&= \int_z q(z\mid x,y) \left[ \log\left(\frac{p(y,z\mid x,\theta)}{q(z\mid x,y)}\right) + \log(q(z\mid x,y)) \right] dz \\
&= \int_z q(z\mid x,y) \log\left(\frac{p(z\mid x,y,\theta) p(y\mid x,\theta)}{q(z\mid x,y)}\right) dz + \mathbb{H}_q \\
&= \int_z q(z\mid x,y) \log(p(y\mid x,\theta)) \, dz - \int_z q(z\mid x,y) \log\left(\frac{q(z\mid x,y)}{p(z\mid x,y,\theta)}\right) dz + \mathbb{H}_q \\
&= \log(p(y\mid x,\theta)) - D_{KL}(q(z\mid x,y) \mid \mid p(z\mid x,y,\theta)) + \mathbb{H}_q
\end{align*}
$$

As the term $$\mathbb{H}_q$$ doesn't depend on $$\theta$$, we have:

$$
\begin{align*}
&\underset{\theta}{argmax}L_{\theta,q}(x,y)=\underset{\theta}{argmax}\mathcal{L} \\
&\mathcal{L}(\theta,q)=L_{\theta,q}(x,y)-\mathbb{H}_q=\log(p(y\mid x,\theta)) - D_{KL}(q(z\mid x,y) \mid \mid p(z\mid x,y,\theta))
\end{align*}
$$

We thus have that $$\mathcal{L}$$, also called the evidence lower bound or ELBO, is a lower bound of the observed data log likelihood.

At time step t, we take $$q^t(z_i\mid x_i,y_i)=p(z_i=k\mid x_i,y_i,\hat{\theta}^t)$$, thus leading to the KL divergence term being 0 for $$\theta=\hat{\theta}^t$$. We note $$\mathcal{L}(\theta,\hat{\theta}^t)=\mathcal{L}(\theta,q^t)$$ and thus have $$\mathcal{L}(\hat{\theta}^t,\hat{\theta}^t)=log(p(y \mid x, \hat{\theta}^t))$$.

This property of having a lower bound that is tight around $$\hat{\theta}^t$$ is the one guaranteeing the increase in the log likelihood as we have:

$$
\begin{align*}
log(p(y|x,\hat{\theta}^{t+1})) &\geq \mathcal{L}(\hat{\theta}^{t+1},\hat{\theta}^{t}) \\
&=\underset{\theta}{max}\mathcal{L}(\theta,\hat{\theta}^{t}) \\
&\geq \mathcal{L}(\hat{\theta}^{t},\hat{\theta}^{t}) \\
&=log(p(y|x,\hat{\theta}^{t}))
\end{align*}
$$.

The EM is thus a part of the MM (Minoration Maximization) class of algorithms, aiming at maximizing a tight minoration of a given objective function. Other important ML algorithms like TRPO (Trust Region Policy Optimization) in the domain of RL rely on this class of methods.

<div class="row justify-content-center" id="fig-2">
    <div class="col-sm-8 mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/mixture_of_experts/MM_illustration.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Illustration of EM, we approximate our non-convex NLL by a lower bound that is tight around $\hat{\theta}^t$ and that is easy to optimize (convex if the experts' NLL is convex). We see graphically that this can only lead to an increase in the log likelihood.
</div>

# Mixture of Experts in Deep Learning and Large Language Models

## Mixture Density Networks (MDN)

In the case of neural networks, using the EM algorithm might not be the most appropriate choice for several reasons:

- Training a neural network is costly, using EM would lead to $$N_{experts} \times N_{EM\_iterations}$$ training procedures introducing a huge overhead.
- One of the strengths of EM is that it keeps the convexity properties of the expert models for the M step. The loss landscape of Neural Networks being non-convex, this property isn't of any interest.

What has instead been proposed in {% cite mixture_density_network %} is to estimate the gating functions using a Neural Network. This approach is of particular interest given the structure of NN as we can have the first part of the model being shared to then add specific heads for each expert and for the gating functions. Such an approach enables to "factorize" computations and to perform inference in one forward pass.

The parameters can then be updated using backpropagation using the negative log likelihood as a loss $$log(p(y \mid x))=log\left(\sum_{k=1}^{N}p(y \mid z=k, x)p(z=k \mid x)\right)$$.

<div class="row justify-content-center" id="fig-2">
    <div class="col-sm-8 mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/mixture_of_experts/MDN.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Illustration of a MDN, the $$\alpha$$ coefficients corresponding to the estimates of p(z=k|x). (Image source: {% cite articulatory_inversion %})
</div>

## Integrating MoEs in Large Language Models: Architecture and Benefits

The previous framing of Mixture of Experts was purely probabilistic and was designed to fit a given probability distribution. Such a notion can however be generalized by seeing MoEs as taking the average of the outputs of each expert $$E_k(x)$$ weighted by the gating functions $$G(x)=(p(z=k \mid x))_k$$. We thus have more generally a way to combine the outputs of various models $$f(x)=\sum_{k=1}^NE_k(x)G(x)_k$$.

In the case of Neural Networks, we can go further and recognize that a Mixture of Experts does not need to be considered only at the model level but actually at the layer level. MoE layers can then be interleaved with other blocks (MLP block, Attention block...) and/or stacked one after the other.

We can even use Mixture of Experts as the experts of an MoE layer leading to hierarchical mixture of experts.

<div class="row justify-content-center" id="fig-2">
    <div class="col-sm-8 mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/mixture_of_experts/sparse_mixture_of_experts_mistral.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Illustration of a mixture of experts layer. We call "router" the part of the model that determines the gating functions. (Image source: {% cite mixtral_of_experts %})
</div>

In the previous section, MoEs were particularly useful as they led to a global model that is more expressive than any of its components (enabling to represent a multimodal distribution using unimodal components). In the case of LLMs, the model already being very expressive (MLPs being universal function approximators), this is not the most useful property.

Mixture of Experts can however turn out to be very useful if we add sparsity constraints to the gating functions. The output of a MoE module being the average of the experts weighted by the gating functions, we have that experts whose gating function is small contribute little to nothing to the final output. We can thus approximate the output by setting the smallest gating functions to zero, thus avoiding the computation of the corresponding expert outputs. This possibility of dynamically choosing which computation to perform is called conditional computation.
The total number of parameters is thus not the same as the number of active parameters during inference.

One common method is to define the final gating function as:

$$
\begin{align*}
&G(x)=Softmax(KeepTopk(H(x),K)) \\
&H(x)=W_g \cdot x \\
&(KeepTopk(v),K)_i=
\begin{cases}
    v_{i}, & \text{if } s_{i,t} \in \text{Topk}(\{v_{i} \mid 1 \leq i \leq N\}, K) \\
    -\infty, & \text{otherwise}
\end{cases}
\end{align*}
$$

The scaling laws {% cite scaling_laws %} tell us that the more parameters a model has, the better it performs. Scaling up the number of parameters, however, leads to a roughly quadratic increase in training compute (linear increase in the computation per forward pass and linear increase in the data to train on). Mixture of Experts enables to scale up the total number of parameters a model has with a subquadratic increase in computation, making them a promising prospect. This is the very reason some of the best LLMs like GPT4, Mixtral_8x7B, or more recently Deepseek V3 have leveraged this architecture.

In practice, mixture of experts are used to replace all the Feed Forward Networks (FFN = MLP with 2 layers) in LLMs but are not used at all for the attention layers.

## Design Choices and Architectural Variations in MoEs

Various models developed throughout the years have developed with different design choices:

| Model                 | Number of Experts (N) | Top-K Routing | Total Parameters | Active Parameters |
|-----------------------|----------------------|--------------|------------------|------------------|
| Switch Transformer    | 128                  | 1            | 26B              | 700M             |
| GShard                | 16                   | 2            | 136B             | 21.6B            |
| Mixtral 8x7B          | 8                    | 2            | 47B              | 13B              |

More recently, DeepSeek introduced DeepseekMoE {% cite deepseekmoe %} which aims at enabling experts to be more specialized (the very reason of mixture of experts) with two distinct modifications:

- Fine-grained experts: In order to have more specialized experts, the authors suggest scaling both N and K by the same multiplicative factor m. And to multiply the inner dimension of the FFN by $$\frac{1}{m}$$ to keep the same overall computational cost. The idea is that by splitting each expert into m smaller experts as this method does, we encourage to have experts that are more highly specialized.

- Shared Expert Isolation: A fixed number of these experts, denoted as the shared experts, will always be gated to. The goal of these experts is to apply any of the common knowledge/information required by the tokens. The idea is that without these shared experts, some different experts will have to acquire this general information leading to redundancies. Using shared experts should lead to less redundancy in the rest of the experts.

With Deepseek MoE has 144.6B total parameters but only 22.2B active parameters.

## Load Balancing Strategies: Ensuring Efficient MoE Training

During the training, the naive MoE architecture might tend to only route responses to a restricted subset of experts leading to a reduction of performance (as we store in memory the weights of the other experts that are effectively useless as they are never called). Such a phenomenon, called routing collapse, comes from the fact that if the routing is focused on an expert at the beginning of the training, this expert will receive more training samples and thus train faster than the other experts, pushing the router to allocate even more resources to this expert.

In order to fix this problem, several strategies have been proposed:

### {% cite sparse_moe %}: Two Differentiable Auxiliary Loss Terms

The goal of the auxiliary loss should be to have a uniform repartition of the "activation" of each expert.
A naive idea would be to define a loss on a batch of size N taking $$\mathcal{L}_{aux}(N_1,...,N_k)$$ with $$N_i=\sum_{x \in \mathcal{B}} \mathbf{1}(\text{x is assigned to Expert i})$$ the number of examples in the batch getting routed to the i-th expert.

The loss could then be any form of loss pushing the $$N_i$$ to be uniform. For example, such a loss could be the square of the coefficient of variation of $$(N_i)_i$$:

$$
\mathcal{L}_{aux}(N_1,...,N_k)=\lambda_{1}CV((N_i)_i))^2=(\frac{\sigma_N}{\mu_N})^2
$$

$$\lambda_{1}$$ being the penalization coefficient.

However, this approach cannot work as the $$N_i$$ are non-differentiable and thus can't be optimized through standard gradient descent.

To resolve this issue, {% cite sparse_moe %} introduces 2 loss terms:

The first one looks like the loss $$\mathcal{L}_{aux}$$ we defined earlier but instead of taking the $$N_i$$ it takes in the $$P_i=\sum_{x \in \mathcal{B}}G_i(x)$$ (called the importance of expert i) as a proxy. Such a proxy helps balancing the load but is far from perfect as we could have a uniform importance among experts while having an imbalanced gating: experts "activated" rarely with high gating weight can have the same importance as experts "activated" frequently with low gating weight. A second auxiliary loss term is thus added in order to complement the 1st one.

In order to introduce the 2nd loss term, we first need to see the modifications to the gating functions made by the paper:

The function $$H(x)$$ which was previously defined as $$W_g \cdot x$$ is now tweaked to have this form:

$$H(x)_i = (W_g \cdot x)_i + \text{StandardNormal}() \cdot \text{Softplus}(W_{\text{noise}}\cdot x)_i$$

with $$Softplus(x)=ln(1+e^x)$$ a smooth approximation of the ReLU.

The general idea is that by making the gating process stochastic, we can consider the differentiable smooth probability of an expert being chosen rather than the non-differentiable and discrete event of actually being chosen. We also introduce a learned scaling factor $$W_{noise}$$ for the normal distribution.

We can then define a smooth proxy for $$N_i$$ as:

$$
\begin{align*}
&Load_i=\sum_{x \in \mathcal{B}}(P_{expert})_i \\
&(P_{expert})_i = \Phi\left(\frac{(x \cdot W_g)_i - \text{kth\_excluding}(H(x), k, i)}{\text{Softplus}((x \cdot W_{\text{noise}})_i)}\right)
\end{align*}
$$

With $$\Phi$$ the CDF (Cumulative Distribution Function) of a Gaussian.

Finally, the 2nd loss term can then be defined as $$\lambda_{2} \cdot CV((Load_i)_i)^2$$.

### {% cite gshard %}: A Single Unified Auxiliary Loss

The previous approach suffers from a few disadvantages:

- It has 2 distinct auxiliary loss trying to enforce load balancing.
- The gating function requires to sample a Gaussian and to train another set of weights $$W_{noise}$$.

In the GShard paper, the authors introduce a single auxiliary loss to balance the load that is both simpler and doesn't require to modify the initial framing of the gating function computation.

As explained earlier, our unattainable goal is to have a loss function $$\mathcal{L}_{aux}(N_1,...,N_k)$$ that is minimized when the $$N_i$$ are uniform.
While the previous loss was the square of the coefficient of variation of the $$N_i$$, another valid loss would be:

$$
\mathcal{L}_{aux}=\sum_{i=0}^{k}N_i \cdot N_i
$$

Indeed, minimizing this quantity is equivalent to the variance of the distribution of the $$N_i$$ and thus is minimized for a uniform distribution.

What can now be done to make this function differentiable is to replace only one of the $$N_i$$ in $$N_i \cdot N_i$$ by the proxy $$P_i=\sum_{x \in \mathcal{B}}G_i(x)$$ leading to a loss function that is differentiable while still taking into account not only the importance but the real load of each expert.

The final loss function can thus be expressed as:

$$
\mathcal{L}_{aux}=\sum_{i=0}^{k}N_i \cdot P_i
$$

### {% cite loss_free_balancing %}: Auxiliary Loss-Free Load Balancing

While improving the load balancing, an auxiliary loss interferes with the training process and may decrease the true model performance.
Indeed, the gradients coming from this loss get backpropagated to $$H(x) = W_g \cdot x$$ modifying $$W_g$$ but also affect the weights of all the layers prior to the gating function. Such a behavior is not desired and is fixed by this paper from DeepSeek.

<div class="row justify-content-center" id="fig-2">
    <div class="col-sm-8 mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/mixture_of_experts/auxiliary_loss_free.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Illustration of the dilemma between performance and load balancing with loss-free balancing breaking this dilemma. The max violation being defined as $$\text{MaxVio} = \frac{\max_i \text{Load}_i - \overline{\text{Load}_i}}{\overline{\text{Load}_i}}$$ (Image source: {% cite loss_free_balancing %})
</div>

The general idea of this method is very simple: We can add biases to the computation of the gating functions and update directly (without backpropagation) these biases to achieve loss balancing.

More formally, the function $$H(x)$$ is defined as:

$$
H(x)=W_g \cdot x + b
$$

For each training batch, we update the biases by counting for each expert the number of tokens they are assigned $$c_i$$ and the number of tokens they should receive if the gating was uniform $$\bar{c_i}$$. We can simply update as $$b_i^{new}=b_i+u*sign(c_i-\bar{c_i})$$.

# Efficient Implementations of Mixture of Experts

We saw that mixture of experts can introduce sparsity, thus reducing the effective number of parameters for a forward pass.
However, implementing efficient kernels for MoEs is tricky and has been one of the driving forces of the recent adoption of MoEs at scale.

## Computation on a Single Device

To avoid getting too early into the details of the more complicated distributed implementation of mixture of experts, we will assume for now that all of the computation is done on a single physical device.

### Batched Matrix Multiplication

The classic approach to compute MoE on hardware is as follows:

<div class="row justify-content-center" id="fig-2">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/mixture_of_experts/moe_layer_batched_mm.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Illustration of the implementation of mixture of experts for $N_{experts}=3$ and $top_k=1$ with token dropping. (Image source: {% cite megablocks %})
</div>

We first compute the output of the gating function giving us the probabilities of each expert. We then permute the tokens so as to group them by the expert they are assigned to and keep in memory this permutation (in the case where $$top_k>1$$ this is not a simple permutation as a token is assigned to multiple experts but can be viewed as a permutation of the tensor where we replicate $$top_k$$ times each token).
Finally, we compute the output of each expert and unpermute the tokens (In the case where $$top_k>1$$ we also compute the average weighted by the gating function outputs for each "duplicated" token).

The primitive used to efficiently compute the output of each expert in parallel is the batched matrix multiplication (BMM) that takes a tensor $$M^1$$ of shape (b,n,m) and $$M^2$$ of shape (b,m,p) and outputs a tensor of shape (b,n,p) where $$output_i=M^1_i@M^2_i$$.

In this case, the matrix $$M_1$$ would store the inputs after permutation with a shape $$(N_{experts},capacity, d)$$ while $$M_2$$ would store the expert weights with shape $$(N_{experts},d, d_{ff})$$ where $$d_{ff}$$ is the width of the middle layer in the FFN (In fact there would be another batched matrix multiply operation with a right operand of shape $$(N_{experts},d_{ff},d)$$ as a FFN corresponds to a MLP with 2 layers).

The curious might have noted that I introduced the capacity in the shape of the first tensor. As the permuted list of tokens should be formatted into a tensor, we need to have the same number of tokens assigned for every expert. This is however not enforced at all during as the routing is dynamic. We need to introduce a quantity that ensures the number of tokens effectively assigned to each expert is fixed.

In order to circumvent this issue, the authors of GShard {% cite gshard %} introduced the notion of expert capacity that can be computed as follows:

$$
\text{expert capacity} = \left( \frac{\text{tokens per batch} \times \text{number of routed experts}}{\text{number of experts}} \right) \times \text{capacity factor}.
$$

The capacity factor is a hyperparameter controlling the "spare room" taken to accommodate an overload of tokens for a given expert. A capacity factor of 1 will make the expert capacity the exact number of tokens each expert should receive under perfectly balanced load. Whenever this load is not completely balanced, there will be token dropping for some overloaded expert and padding for other underloaded experts. While token dropping is not dramatic thanks to the residual connections (the hidden representation of a dropped token will just remain unchanged), token dropping still negatively affects performances and should be avoided.

We can see that there is a very subtle dilemma between generating less token dropping & better model performance with a higher capacity factor and reducing the computational overhead with a lower capacity. Such a problem highlights once again how important effective load balancing is for MoEs.

### Block Sparse Matrix Multiplication

To circumvent this issue of token dropping, Megablocks {% cite megablocks %} developed new optimized CUDA primitives for blocked sparse matrix multiplication.

<div class="row justify-content-center" id="fig-2">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/mixture_of_experts/block_sparse_mm.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Illustration of the implementation of block sparse matrix multiplication at the right. This enables to have variable number of tokens allocated to experts and even to have experts of different sizes. (Image source: {% cite megablocks %})
</div>

For simplicity, we will assume that experts have the same size (nearly always the choice made in practice).

More formally, we can see that with this primitive we do not have to stack the inputs of each expert along a new dimension (leading to a tensor of size $$(N_{experts},capacity, d)$$ as shown previously) but we can simply have a 2d input x of size (N_{tokens},d) that is permuted such that the first rows are mapped to expert 1 and so on... On the other hand, the expert matrix $$W_E$$ is of binary size $$(N_{experts}d_{ff},d)$$ and the global output matrix (after passing through the 1st of the 2 FFN layers) $$out=x \cdot W_E^T$$ will be of size $$(N_{tokens},N_{experts}d_{ff})$$.

We however do not care about most of the elements in the output matrix as for a given token we only want the result of the experts it is matched to. To make it more clear, the dense matrix described above will have the columns $$1,...,d_{ff}$$ corresponding to the computation of the 1st expert, the columns $$d_{ff}+1, ...,2d_{ff}$$ corresponding to the 2nd expert and so on.

This is the exact framing of an SDD (Sparse Dense Dense) block sparse matrix multiplication where we multiply 2 dense matrices to yield a sparse matrix. This sparsity constraint needs to be given through a binary mask that will equal to 1 for blocks we want to compute and 0 otherwise.
More precisely, the binary mask corresponding to the position (i,j) will be equal to 1 iff the i-th token is gated to the expert associated to the j-th column.

The overall procedure is to tile the matrix into blocks and perform the block multiplication iff one of the values of the mask is 1 for this block. After the computation, we mask again the output as a block containing 0 and 1s will be computed and should have some of these values masked.

#### Why Use Block Sparsity Instead of Sparsity?

Computing a masked output introduces at some point or another a conditional statement as we want to perform the computation only if the value of the mask is 1. As conditional statements are way slower on GPUs than arithmetic operations, we want to perform as much operations per if statement as possible by increasing the block size to achieve peak performance and arithmetic intensity. On the other extreme, if the block size is the size of the matrix, the global operation consists in performing the very inefficient full dense matrix multiplication to then apply the mask. The block size should then be tuned with care to ensure peak performance.

<div class="row justify-content-center" id="fig-2">
    <div class="col-sm-8 mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/mixture_of_experts/sparse_vs_block_sparse.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Illustration of sparse vs block sparse matrices. (Image source: <a href="https://andrew.gibiansky.com/wavernn-demystified-sparsity/">Andrew Gibiansky website</a>)
</div>

#### Why Permute and Unpermute Tokens?

This block sparse matrix multiplication would theoretically work without permuting the tokens. However, this would remove all the structure in the output mask as we would not have anymore all the first tokens mapped to the 1st expert, the following mapped to the 2nd expert. This would thus lead to a sparse matrix that isn't block sparse at all affecting very negatively performances.

## Distributed MoEs: Scaling and Parallelism Strategies

Having large batch sizes is important for compute efficiency during training as they enable to perform more computation each time the matrix weights are moved from the slow HBM (High Bandwidth Memory) to each of the Streaming Multiprocessor (SM) shared memory, thus increasing the arithmetic intensity.

However, MoE reduces the effective batch size of each expert by a factor of $$\frac{K}{N_{experts}}$$ leading to suboptimal computations.

We will see in this section how this problem can be treated when distributing training on several devices.

### Data Parallelism

In the case of MoE, a naive approach to distribute training on several machines is to equally divide each batch into mini-batches where each mini-batch is sent to a different device that stores the totality of the model weights.

This approach is very straightforward and introduces very little communication overhead as you only need to transmit information between devices at the very end when collecting the gradients (this collecting operation is in fact a <a href="https://en.wikipedia.org/wiki/Reduction_operator">reduction operation</a> where we sum the gradients transmitted by each device).

<div class="row justify-content-center" id="fig-2">
    <div class="col-sm-8 mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/mixture_of_experts/reduce_operation.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Illustration of a reduce operation.
</div>

### Model Parallelism

As LLMs are now too big to fit on a single machine, Model Parallelism (MP) splits the model weights among $$m$$ machines. MP acts like an horizontal cut in the model weights where each matrix is divided into little parts so that at each matrix multiplication the computational load will be balanced between the $$m$$ devices. More precisely, each device uses only $$\frac{d}{m}$$ of the hidden dimensions to compute its output and the output of each device are then summed together through an all_reduce operation.

<div class="row justify-content-center" id="fig-2">
    <div class="col-sm-8 mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/mixture_of_experts/all_reduce.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Illustration of an all_reduce operation.
</div>

We note that the implementation of the reduce or all_reduce message passing primitives can themselves be optimized based on the topology of the cluster (how each machine is linked to the others), we choose to not dive this deep.

Model and Data Parallelism are often used together in what is sometimes called FSDP (Fully Sharded Data Parallelism).

### Expert Parallelism

Expert parallelism simply refers to the idea of storing different subsets of experts on different devices. In this case, the gating operation is in fact really routing the tokens to different machines.

Such a notion highlights once again the need for load balancing as an unbalanced gating function would lead to some devices being idle while others are overloaded diminishing the overall efficiency.

### Expert + Model + Data Parallelism

One can use a clever trick to roughly maintain the same batch size for experts and for the rest of the model.
If we perform Fully Sharded Data Parallelism with $$\frac{N_{experts}}{K}$$ mini-batches of size $$\mathcal{B}$$ and then apply expert parallelism on the outputs of all the mini-batches. As we saw previously, a balanced load leads to each expert receiving $$\frac{K}{N_{experts}}$$ of the total examples so $$\frac{K}{N_{experts}}(\frac{N_{experts}}{K}\mathcal{B})=\mathcal{B}$$ examples.

<div class="row justify-content-center" id="fig-2">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/mixture_of_experts/expert_parallelism.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    An illustration of Data Parallelism, Expert Parallelism, and Model Parallelism. Each square corresponds to distinct copies of the weights and each color corresponds to different experts to assign to. (Image source: {% cite switch_transformer %})
</div>

# References

{% bibliography --cited %}
