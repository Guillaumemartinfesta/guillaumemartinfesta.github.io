---
---

@article{ml_probabilistic_perspective,
  abbr={Ann. Phys.},
  title={MAchine Learning: A Probabilistic Perspective},
  author={Kevin P. Murphy},
  year={2012}
}

@article{mixtral_of_experts,
  abbr={Ann. Phys.},
  title={Mixtral Of Experts},
  author={Albert Q. Jiang et al.},
  year={2024},
  url={https://arxiv.org/pdf/2401.04088}
}
@article{deepseekmoe,
  title={DeepSeekMoE: Towards Ultimate Expert Specialization in
Mixture-of-Experts Language Models},
  author={Damai Dai et al.},
  year={2024},
  url={https://arxiv.org/pdf/2401.06066}
}

@article{sparse_moe,
  title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
  author={Noam Shazeer et al.},
  year={2017},
  url={https://arxiv.org/pdf/1701.06538}
}
@article{loss_free_balancing,
  title={Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts},
  author={Lean Wang et al.},
  year={2024},
  url={https://arxiv.org/pdf/2408.15664}
}

AUXILIARY-LOSS-FREE LOAD BALANCING STRATEGY FOR MIXTURE-OF-EXPERTS
