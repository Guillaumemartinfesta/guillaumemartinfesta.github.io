<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://guillaumemartinfesta.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://guillaumemartinfesta.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-02-11T11:39:49+00:00</updated><id>https://guillaumemartinfesta.github.io/feed.xml</id><title type="html">blank</title><entry><title type="html">Mixture of Experts: From the ground up to use in LLMs.</title><link href="https://guillaumemartinfesta.github.io/blog/2024/my-template-post/" rel="alternate" type="text/html" title="Mixture of Experts: From the ground up to use in LLMs."/><published>2024-10-02T00:00:00+00:00</published><updated>2024-10-02T00:00:00+00:00</updated><id>https://guillaumemartinfesta.github.io/blog/2024/my-template-post</id><content type="html" xml:base="https://guillaumemartinfesta.github.io/blog/2024/my-template-post/"><![CDATA[<h1 id="fondations-of-mixture-of-experts-moe">Fondations of Mixture of Experts (MoE)</h1> <h2 id="problem-description">Problem Description</h2> <p>Let’s say we want to fit this distribution of y given x:</p> <p>We first introduce the direct problem:</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mixture_of_experts/direct_problem-480.webp 480w,/assets/img/mixture_of_experts/direct_problem-800.webp 800w,/assets/img/mixture_of_experts/direct_problem-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mixture_of_experts/direct_problem.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Direct problem distribution </div> <p>As we will see shortly after we swaped the x and y axis as the problem of interest will actually be the inverse of this one. We can see here that predicting x given y corresponds to predicting a trend x=f(y) and accounting for some measuring noise. As the trend is a function mapping from y to x (with only one x associated to a given y) the distribution we are trying to predict is unimodal, a sensible model to fit to this data would thus have this form:</p> <p>$y \sim \mathcal{N}(\mu(x \mid \theta),\sigma^2)$</p> <p>The parameters to fit for this model are the parameters $\theta$ of $\mu(x \mid \theta)$ estimating the mode/trend and $\sigma^2$ accounting for the residual noise. $\mu(x \mid \theta)$ could be any sort of model: linear regression, polynomial regression, kernel based model, neural network…</p> <p>This problem description is very classic and enables to simply fit by trying to minimize the NLL(Negative Log Likelihood).</p> <p>However considering the inverse problem by predicting y as a function of x does not lead to such nice properties:</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mixture_of_experts/inverse_problem-480.webp 480w,/assets/img/mixture_of_experts/inverse_problem-800.webp 800w,/assets/img/mixture_of_experts/inverse_problem-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mixture_of_experts/inverse_problem.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Inverse problem distribution </div> <p>While the direct problem led to a simple unimodal distribution, we see that the inverse problem has 2 modes for x between 0.4 and 0.6 making the previous model obsolete. We see that such a phenomenon of a multimodal inverse problem arises whenever we consider the inverse of a function that is not injective.</p> <p>Luckily Mixture of Experts(MoE) is a type of models that enables to solve this issue. MoEs belong to the class Latent Variable Models(LVM) that rely on some hidden variables (the latent variables) to make predictions.</p> <p>More formally we could introduce a categorical latent variable z such that $p(z\mid x)=Cat(z\mid Softmax(V^T(1,x)))$ that will determine the “expert” to use.The function $p(z\mid x)$ is called the gating function. Each of the N experts can then be a model of the form described earlier $p(y\mid x,z=k)=\mathcal{N}(y\mid\mu_k(x \mid \theta_k),\sigma_k^2)$ describing an unimodal distribution. As for a given x we could have a nonzero probability of “activating” each expert, we can then describe any distribution having less than N modes.</p> <p>The model parameters are then $\theta_1,…,\theta_k,\sigma_1^2,…,\sigma_k^2$ and $V \in \mathcal{M}_{2 \times N}$. We denote by $\theta$ all these parameters.</p> <h2 id="fitting-moes-the-em-algorithm">Fitting MoEs: the EM algorithm</h2> <p>Fitting a Latent Variable Model is not straight forward. The joint log probability $log(p(y,z\mid x,\theta))$ is easy to compute but the observed data log likelihood is hard to compute since $log(p(y\mid x,\theta))=log(\sum p(y,z\mid x,theta))$.</p> <p>The log can’t be pushed into the sum. Moreover most of the simple models lead to a convex log likelihood but this property is lost for LVM as the logarithm of the sum of 2 log convex functions is not necessarly convex making the optimization problem much harder.</p> <p>As the complete data log probability is easy to compute we estimate the observed data log likelihood log(p(x)) by the expected complete data log likelihood $\mathbb{E}_{z \sim q(z\mid x,y)}[log(P(y,z\mid x))]$</p> <p>The EM algorithm does exactly that in order by alternating between 2 steps:</p> <ul> <li> <p>The E (Estimation) step where we estimate $q^t(z_i\mid x_i,y_i)=p(z_i=k\mid x_i,y_i,\hat{\theta}^t)$</p> </li> <li> <p>The M (Maximization) step were we compute $\hat{\theta}^{t+1}=\underset{\theta}{argmax}\sum_{i=1}^n\mathbb{E}_{z_i \sim q^t(z_i\mid x_i,y_i)}[log(P(y_i,z_i\mid x_i))]$</p> </li> </ul> <p>In other words the EM algorithm iteratively computes $\hat{\theta}^{t+1}=\underset{\theta}{argmax}\sum_{i=1}^n\sum_{k=1}^Nlog[p(y_i,z_i=k\mid x_i,\theta)]p(z_i=k\mid x_i,\hat{\theta^t})$</p> <p>This method can be used whenever we can approximate the posterior probability in the E step. It is however also necessary to be able to fit rapidly the expert models (for example by having closed form formulas or a convex NLL) as we will have to fit the N experts for every iteration.</p> <h2 id="theoretical-proofs-for-the-em-algorithm">Theoretical proofs for the EM algorithm</h2> <p>We will show in this part that the EM algorithm monotically increases the observed data log likelihood.</p> <p>Our first goal is to derive a formula showing how far of our estimation of the observed data log likelihood by the expected complete data log likelihood is.</p> <p>To make the point more general we assume the variable z is continuous (the same results also hold for discrete z)</p> \[\begin{align*} L_{\theta,q}(x,y)=\mathbb{E}_{z \sim q(z\mid x,y)}[\log(p(y,z\mid x,\theta))] &amp;= \int_z q(z\mid x,y) \log(p(y,z\mid x,\theta)) \, dz \\ &amp;= \int_z q(z\mid x,y) \left[ \log\left(\frac{p(y,z\mid x,\theta)}{q(z\mid x,y)}\right) + \log(q(z\mid x,y)) \right] dz \\ &amp;= \int_z q(z\mid x,y) \log\left(\frac{p(z\mid x,y,\theta) p(y\mid x,\theta)}{q(z\mid x,y)}\right) dz + \mathbb{H}_q \\ &amp;= \int_z q(z\mid x,y) \log(p(y\mid x,\theta)) \, dz - \int_z q(z\mid x,y) \log\left(\frac{q(z\mid x,y)}{p(z\mid x,y,\theta)}\right) dz + \mathbb{H}_q \\ &amp;= \log(p(y\mid x,\theta)) - D_{KL}(q(z\mid x,y) \mid \mid p(z\mid x,y,\theta)) + \mathbb{H}_q \end{align*}\] <p>As the term $\mathbb{H}_q$ doesn’t depend on $\theta$ we have:</p> \[\begin{align} &amp;\underset{\theta}{argmax}L_{\theta,q}(x,y)=\underset{\theta}{argmax}\mathcal{L} \\ &amp;\text{with :} \mathcal{L}(\theta,q)=L_{\theta,q}(x,y)-\mathbb{H}_q=\log(p(y\mid x,\theta)) - D_{KL}(q(z\mid x,y) \mid \mid p(z\mid x,y,\theta)) \text{the evidence lower bound or ELBO.} \end{align}\] <p>As its name suggests the ELBO is a lower bound of the observed data log likelihood.</p> <p>At time step t we take $q^t(z_i\mid x_i,y_i)=p(z_i=k\mid x_i,y_i,\hat{\theta}^t)$ thus leading to the KL divergence term being 0 for $\theta=\hat{\theta}^t$. We note $\mathcal{L}(\theta,\hat{\theta}^t)=\mathcal{L}(\theta,q^t)$</p> <p>This property of having a lower bound that is tight around $\hat{\theta}^t$ is the one guaranteeing the increase in the log likelihood as we have:</p> <p>\(\begin{align} log(p(y|x,\hat{\theta}^{t+1})) &amp;\geq \mathcal{L}(\hat{\theta}^t,\hat{\theta}^{t+1}) \\ &amp;\geq \mathcal{L}(\hat{\theta}^t,\hat{\theta}^{t+1}) \\ &amp;=log(p(y|x,\hat{\theta}^{t})) \end{align}\).</p> <p>The EM is thus apart of the MM(Minoration Maximization) class of algorithms, aiming at maximizing a tight minoration of a given objective function. Other important ML algorithms like TRPO(Trust Region Policy Optimization) in the domain of RL rely on this class of methods.</p> <h1 id="use-of-moes-in-deep-learning-and-llms">Use of MoEs in Deep Learning and LLMs</h1> <h2 id="mixture-density-networks-mdn">Mixture Density Networks (MDN)</h2> <p>In the case of neural networks, using the EM algorithm might not be the most appropriate choice for several reasons:</p> <ul> <li>Training a neural network is costly, using EM would lead to $N_{experts} \times N_{EM_iterations}$ training procedures introducing a huge overhead.</li> <li>One of the strength of EM is that it keeps the convexity properties of the expert models for the M step. The loss landscape of Neural Networks being non convex this property isn’t of any interest.</li> </ul> <p>What we can instead do is to estimate the gating functions using a Neural Network. This approach is of particular interest given the structure of NN as we can have the first part of the model being shared to then add specific heads for each expert and for the gating functions. Such an approach enables to “factorize” computations and to perform inference in one forward pass.</p> <p>The parameters can then be updated using with backpropagation using the negative log likelihood as a loss $log(p(y \mid x))=log\left(\sum_{k=1}^{N}p(y \mid z=k, x)p(z=k \mid x)\right)$</p> <p>TODO: Insert picture of architecture</p> <p>Thanks to the flexibility of this approach we can generalize MoEs as taking the average of the outputs of each expert $f_k(x)$ weighted by the gating functions $p(z=k \mid x)$ instead of only considering the probabilistic approach. We thus have more generally $f(x)=\sum_{k=1}^Nf_k(x)p(z=k \mid x)$</p> <p>Finally we can define Mixture of Experts at the layer level enabling them to be interleaved with other blocks (MLP block, Attention block…) and/or to be stacked one after the other.</p> <h2 id="use-of-moes-in-llm">Use of MoEs in LLM</h2> <p>The initial framing of mixture of experts was to be able to represent multimodal distribution for $p(y \mid x)$. This is however of no use for LLMs as they directly output the probability distribution corresponding to the next token.</p> <p>Mixture of Experts can however turn out to be very useful if we add sparsity constraints to the gating functions. The output of a the MoE module being the average of the experts weighted by the gating functions, we have that experts whose gating function is small contribute little to nothing to the final output. We can thus approximate the output by setting the smallest gating functions to zero thus avoiding the computation of the corresponding expert outputs.</p> <p>One common method is to use as a final gating function $Softmax(Top_K(p(z=k)_{k \in [1:N]}))$ where we only take the k highest gating function logits (pre softmax) before applying the softmax to them.</p> <p>Test:</p> <p>(missing reference)</p> <h1 id="references">References</h1>]]></content><author><name></name></author><category term="llm"/><category term="formatting"/><category term="links"/><summary type="html"><![CDATA[Fondations of Mixture of Experts (MoE)]]></summary></entry></feed>