<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://guillaumemartinfesta.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://guillaumemartinfesta.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-02-11T20:56:03+00:00</updated><id>https://guillaumemartinfesta.github.io/feed.xml</id><title type="html">blank</title><entry><title type="html">Mixture of Experts: From the ground up to use in LLMs.</title><link href="https://guillaumemartinfesta.github.io/blog/2024/moe/" rel="alternate" type="text/html" title="Mixture of Experts: From the ground up to use in LLMs."/><published>2024-10-02T00:00:00+00:00</published><updated>2024-10-02T00:00:00+00:00</updated><id>https://guillaumemartinfesta.github.io/blog/2024/moe</id><content type="html" xml:base="https://guillaumemartinfesta.github.io/blog/2024/moe/"><![CDATA[<h1 id="fondations-of-mixture-of-experts-moe">Fondations of Mixture of Experts (MoE)</h1> <p>This section is inspired by the Chapter 11 of the amazing book Machine Learning: A probabilistic perspective by Kevin P. Murphy</p> <h2 id="problem-description">Problem Description</h2> <p>Let’s say we want to fit this distribution of y given x:</p> <p>We first introduce the direct problem:</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mixture_of_experts/direct_problem-480.webp 480w,/assets/img/mixture_of_experts/direct_problem-800.webp 800w,/assets/img/mixture_of_experts/direct_problem-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mixture_of_experts/direct_problem.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Direct problem distribution </div> <p>As we will see shortly after we swaped the x and y axis as the problem of interest will actually be the inverse of this one. We can see here that predicting x given y corresponds to predicting a trend x=f(y) and accounting for some measuring noise. As the trend is a function mapping from y to x (with only one x associated to a given y) the distribution we are trying to predict is unimodal, a sensible model to fit to this data would thus have this form:</p> \[y \sim \mathcal{N}(\mu(x \mid \theta),\sigma^2)\] <p>The parameters to fit for this model are the parameters \(\theta\) of \(\mu(x \mid \theta)\) estimating the mode/trend and \(\sigma^2\) accounting for the residual noise. \(\mu(x \mid \theta)\) could be any sort of model: linear regression, polynomial regression, kernel based model, neural network…</p> <p>This problem description is very classic and enables to simply fit by trying to minimize the NLL(Negative Log Likelihood).</p> <p>However considering the inverse problem by predicting y as a function of x does not lead to such nice properties:</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mixture_of_experts/inverse_problem-480.webp 480w,/assets/img/mixture_of_experts/inverse_problem-800.webp 800w,/assets/img/mixture_of_experts/inverse_problem-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mixture_of_experts/inverse_problem.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Inverse problem distribution </div> <p>While the direct problem led to a simple unimodal distribution, we see that the inverse problem has 2 modes for x between 0.4 and 0.6 making the previous model obsolete. We see that such a phenomenon of a multimodal inverse problem arises whenever we consider the inverse of a function that is not injective.</p> <p>Luckily Mixture of Experts(MoE) is a type of models that enables to solve this issue. MoEs belong to the class Latent Variable Models(LVM) that rely on some hidden variables (the latent variables) to make predictions.</p> <p>More formally we could introduce a categorical latent variable z such that \(p(z\mid x)=Cat(z\mid Softmax(V^T(1,x)))\) that will determine the “expert” to use.The function \(p(z\mid x)\) is called the gating function. Each of the N experts can then be a model of the form described earlier \(p(y\mid x,z=k)=\mathcal{N}(y\mid\mu_k(x \mid \theta_k),\sigma_k^2)\) describing an unimodal distribution. As for a given x we could have a nonzero probability of “activating” each expert, we can then describe any distribution having less than N modes.</p> <p>The model parameters are then \(\theta_1,...,\theta_k,\sigma_1^2,...,\sigma_k^2\) and \(V \in \mathcal{M}_{2 \times N}\). We denote by \(\theta\) all these parameters.</p> <h2 id="fitting-moes-the-em-algorithm">Fitting MoEs: the EM algorithm</h2> <p>Fitting a Latent Variable Model is not straight forward. The joint log probability \(log(p(y,z\mid x,\theta))\) is easy to compute but the observed data log likelihood is hard to compute since \(log(p(y\mid x,\theta))=log(\sum p(y,z\mid x,theta))\).</p> <p>The log can’t be pushed into the sum. Moreover most of the simple models lead to a convex log likelihood but this property is lost for LVM as the logarithm of the sum of 2 log convex functions is not necessarly convex making the optimization problem much harder.</p> <p>As the complete data log probability is easy to compute we estimate the observed data log likelihood log(p(x)) by the expected complete data log likelihood \(\mathbb{E}_{z \sim q(z\mid x,y)}[log(P(y,z\mid x))]\)</p> <p>The EM algorithm does exactly that in order by alternating between 2 steps:</p> <ul> <li> <p>The E (Estimation) step where we estimate \(q^t(z_i\mid x_i,y_i)=p(z_i=k\mid x_i,y_i,\hat{\theta}^t)\)</p> </li> <li> <p>The M (Maximization) step were we compute \(\hat{\theta}^{t+1}=\underset{\theta}{argmax}\sum_{i=1}^n\mathbb{E}_{z_i \sim q^t(z_i\mid x_i,y_i)}[log(P(y_i,z_i\mid x_i))]\)</p> </li> </ul> <p>In other words the EM algorithm iteratively computes \(\hat{\theta}^{t+1}=\underset{\theta}{argmax}\sum_{i=1}^n\sum_{k=1}^Nlog[p(y_i,z_i=k\mid x_i,\theta)]p(z_i=k\mid x_i,\hat{\theta^t})\)</p> <p>This method can be used whenever we can approximate the posterior probability in the E step. It is however also necessary to be able to fit rapidly the expert models (for example by having closed form formulas or a convex NLL) as we will have to fit the N experts for every iteration.</p> <h2 id="theoretical-proofs-for-the-em-algorithm">Theoretical proofs for the EM algorithm</h2> <p>We will show in this part that the EM algorithm monotically increases the observed data log likelihood.</p> <p>Our first goal is to derive a formula showing how far of our estimation of the observed data log likelihood by the expected complete data log likelihood is.</p> <p>To make the point more general we assume the variable z is continuous (the same results also hold for discrete z)</p> \[\begin{align*} L_{\theta,q}(x,y)=\mathbb{E}_{z \sim q(z\mid x,y)}[\log(p(y,z\mid x,\theta))] &amp;= \int_z q(z\mid x,y) \log(p(y,z\mid x,\theta)) \, dz \\ &amp;= \int_z q(z\mid x,y) \left[ \log\left(\frac{p(y,z\mid x,\theta)}{q(z\mid x,y)}\right) + \log(q(z\mid x,y)) \right] dz \\ &amp;= \int_z q(z\mid x,y) \log\left(\frac{p(z\mid x,y,\theta) p(y\mid x,\theta)}{q(z\mid x,y)}\right) dz + \mathbb{H}_q \\ &amp;= \int_z q(z\mid x,y) \log(p(y\mid x,\theta)) \, dz - \int_z q(z\mid x,y) \log\left(\frac{q(z\mid x,y)}{p(z\mid x,y,\theta)}\right) dz + \mathbb{H}_q \\ &amp;= \log(p(y\mid x,\theta)) - D_{KL}(q(z\mid x,y) \mid \mid p(z\mid x,y,\theta)) + \mathbb{H}_q \end{align*}\] <p>As the term \(\mathbb{H}_q\) doesn’t depend on \(\theta\) we have:</p> \[\begin{align} &amp;\underset{\theta}{argmax}L_{\theta,q}(x,y)=\underset{\theta}{argmax}\mathcal{L} \\ &amp;\mathcal{L}(\theta,q)=L_{\theta,q}(x,y)-\mathbb{H}_q=\log(p(y\mid x,\theta)) - D_{KL}(q(z\mid x,y) \mid \mid p(z\mid x,y,\theta)) \end{align}\] <p>We thus have that \(\mathcal{L}\) also called the evidence lower bound or ELBO. is a lower bound of the observed data log likelihood.</p> <p>At time step t we take \(q^t(z_i\mid x_i,y_i)=p(z_i=k\mid x_i,y_i,\hat{\theta}^t)\) thus leading to the KL divergence term being 0 for \(\theta=\hat{\theta}^t\) and We note \(\mathcal{L}(\theta,\hat{\theta}^t)=\mathcal{L}(\theta,q^t)\) and thus have \(\mathcal{L}(\hat{\theta}^t,\hat{\theta}^t)=log(p(y \mid x, \hat{\theta}^t))\)</p> <p>This property of having a lower bound that is tight around \(\hat{\theta}^t\) is the one guaranteeing the increase in the log likelihood as we have:</p> <p>\(\begin{align} log(p(y|x,\hat{\theta}^{t+1})) &amp;\geq \mathcal{L}(\hat{\theta}^{t+1},\hat{\theta}^{t}) \\ &amp;=\underset{\theta}{max}\mathcal{L}(\theta,\hat{\theta}^{t}) \\ &amp;\geq \mathcal{L}(\hat{\theta}^{t},\hat{\theta}^{t}) \\ &amp;=log(p(y|x,\hat{\theta}^{t})) \end{align}\).</p> <p>The EM is thus apart of the MM(Minoration Maximization) class of algorithms, aiming at maximizing a tight minoration of a given objective function. Other important ML algorithms like TRPO(Trust Region Policy Optimization) in the domain of RL rely on this class of methods.</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mixture_of_experts/MM_illustration-480.webp 480w,/assets/img/mixture_of_experts/MM_illustration-800.webp 800w,/assets/img/mixture_of_experts/MM_illustration-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mixture_of_experts/MM_illustration.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Illustration of EM, we approximate our non concave log likelihood by a lower bound that is tight around $$\hat{\theta}^t$$ and that is easy to optimize (concave if the log likelihood of the experts is concave). We see graphically that this can only lead to an increase in the log likelihood. </div> <h1 id="use-of-moes-in-deep-learning-and-llms">Use of MoEs in Deep Learning and LLMs</h1> <h2 id="mixture-density-networks-mdn">Mixture Density Networks (MDN)</h2> <p>In the case of neural networks, using the EM algorithm might not be the most appropriate choice for several reasons:</p> <ul> <li>Training a neural network is costly, using EM would lead to \(N_{experts} \times N_{EM\_iterations}\) training procedures introducing a huge overhead.</li> <li>One of the strength of EM is that it keeps the convexity properties of the expert models for the M step. The loss landscape of Neural Networks being non convex this property isn’t of any interest.</li> </ul> <p>What has instead been proposed in <a class="citation" href="#mixture_density_network">(Bishop, 2024)</a> is to estimate the gating functions using a Neural Network. This approach is of particular interest given the structure of NN as we can have the first part of the model being shared to then add specific heads for each expert and for the gating functions. Such an approach enables to “factorize” computations and to perform inference in one forward pass.</p> <p>The parameters can then be updated using with backpropagation using the negative log likelihood as a loss \(log(p(y \mid x))=log\left(\sum_{k=1}^{N}p(y \mid z=k, x)p(z=k \mid x)\right)\)</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mixture_of_experts/MDN-480.webp 480w,/assets/img/mixture_of_experts/MDN-800.webp 800w,/assets/img/mixture_of_experts/MDN-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mixture_of_experts/MDN.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Illustration of a MDN, the $$\alpha$$ coeficients corresponding to the estimates of p(z=k|x). (Image source: <a class="citation" href="#articulatory_inversion">(Uria, 2012)</a>) </div> <h2 id="use-of-moes-in-llm">Use of MoEs in LLM</h2> <p>TODO: Revamp</p> <p>[ Thanks to the flexibility of this approach we can generalize MoEs as taking the average of the outputs of each expert \(E_k(x)\) weighted by the gating functions \(G(x)=(p(z=k \mid x))_k\) instead of only considering the probabilistic approach. We thus have more generally \(f(x)=\sum_{k=1}^NE_k(x)G(x)_k\)</p> <p>Finally we can define Mixture of Experts at the layer level enabling them to be interleaved with other blocks (MLP block, Attention block…) and/or to be stacked one after the other. ]</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mixture_of_experts/sparse_mixture_of_experts_mistral-480.webp 480w,/assets/img/mixture_of_experts/sparse_mixture_of_experts_mistral-800.webp 800w,/assets/img/mixture_of_experts/sparse_mixture_of_experts_mistral-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mixture_of_experts/sparse_mixture_of_experts_mistral.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Illustration of a mixture of experts layer. We call "router" the part of the model that determines that gating functions. (Image source: <a class="citation" href="#mixtral_of_experts">(Jiang, 2024)</a>) </div> <p>The initial framing of mixture of experts was to be able to represent multimodal distribution for \(p(y \mid x)\). This is however of no use for LLMs as they directly output the probability distribution corresponding to the next token.</p> <p>Mixture of Experts can however turn out to be very useful if we add sparsity constraints to the gating functions. The output of a the MoE module being the average of the experts weighted by the gating functions, we have that experts whose gating function is small contribute little to nothing to the final output. We can thus approximate the output by setting the smallest gating functions to zero thus avoiding the computation of the corresponding expert outputs.</p> <p>One common method is to define the final gating function as: \(\begin{align} &amp;G(x)=Softmax(KeepTopk(H(x),K)) \\ &amp;H(x)=W_g \cdot x \\ &amp;(KeepTopk(v),K)_i= \begin{cases} v_{i}, &amp; \text{if } s_{i,t} \in \text{Topk}(\{v_{i} \mid 1 \leq i \leq N\}, K) \\ -\infty, &amp; \text{otherwise} \end{cases} \end{align}\)</p> <h2 id="load-balancing-strategies">Load balancing strategies</h2> <p>During the training the naive MoE architecture, the model might tend to only route responses to a restricted subset of experts leading to a reduction of performance (as we store in memory the weights of the other experts that are effectively useless as they are never called). Such a phenomenon called routing collapse comes from the fact that if the routing is focused at an expert at the beginning of the training, this expert will receive more training samples and thus train faster than the other experts pushing the router to allocate even more ressources to this expert.</p> <p>In order to fix this problem, several strategies have been proposed:</p> <h3 id="shazeer-2017-2-differentiable-auxiliary-loss-terms"><a class="citation" href="#sparse_moe">(Shazeer, 2017)</a>: 2 differentiable auxiliary loss terms</h3> <p>The goal of the auxiliary loss should be to have a uniform repartition of the “activation” of each expert. A naive idea would be te to thus to define a loss on a batch of size N taking \(\mathcal{L}_{aux}(N_1,...,N_k)\) with \(N_i=\sum_{x \in \mathcal{B}} \mathbf{1}(\text{x is assigned to Expert i})\) the number of examples in the batch getting routed to the i-th expert.</p> <p>The loss could then be any form of loss pushing the \(N_i\) to be uniform. For example such a loss could be the square of coeficient of variation of \((N_i)_i\) \(\mathcal{L}_{aux}(N_1,...,N_k)=\lambda_{1}CV((N_i)_i))^2=(\frac{\sigma_N}{\mu_N})^2\), \(lambda_{1}\) being the penalization coeficent.</p> <p>However this approach cannot work as the \(N_i\) are non differentiable and thus can’t be optimized through standart gradient descent.</p> <p>To resolve this issue <a class="citation" href="#sparse_moe">(Shazeer, 2017)</a> introduces 2 loss terms:</p> <p>The first one looks like the loss \(\mathcal{L}_{aux}\) we defined earlier but instead of taking the \(N_i\) it takes in the \(P_i=\sum_{x \in \mathcal{B}}G_i(x)\)$ (called the importance of expert i) as a proxy. Such a proxy helps balancing the load but is far from perfect as we could have a uniform importance among experts while having an imbalanced gating: experts “activated” rarely with high gating weight can have the same importance as experts “activated” frequently with low gating weight. A second auxiliarry loss term is thus added in order to complement the 1st one.</p> <p>In order to introduce the 2nd loss term we first need to see the modifications to the gating functions made by the paper:</p> <p>The function \(H(x)\) which was previously defined as \(W_g \cdot x\) is now tweaked to have this form:</p> \[H(x)_i = (W_g \cdot x)_i + \text{StandardNormal}() \cdot \text{Softplus}(W_{\text{noise}}\cdot x)_i\] <p>with \(Softplus(x)=ln(1+e^x)\) a smooth approximation of the ReLU.</p> <p>The general idea is that, by making the gating process stochastic, we can consider the differentiable smooth probability of an expert being chosen rather than the non-differentiable and discrete event of actually being chosen. We also introduce a learned scaling factor \(W_{noise}\) for the normal distribution.</p> <p>We can then define a smooth proxy for \(N_i\) as : \($ \begin{align} &amp;Load_i=\sum_{x \in \mathcal{B}}(P_{expert})_i \\ &amp;(P_{expert})_i = \Phi\left(\frac{(x \cdot W_g)_i - \text{kth\_excluding}(H(x), k, i)}{\text{Softplus}((x \cdot W_{\text{noise}})_i)}\right) \end{align}\)$ With \(\Phi\) the CDF(Cumulative Distribution Function) of a Gaussian.</p> <p>Finally the 2nd loss term can then be defined as \(\lambda_{2} \cdot CV((Load_i)_i)^2\)</p> <h3 id="lepikhin-2020-a-single-unified-auxiliary-loss"><a class="citation" href="#gshard">(Lepikhin, 2020)</a>: A single unified auxiliary loss</h3> <p>The previous approach suffers from a few disadvantages:</p> <ul> <li>It has 2 distinct auxiliary loss trying to enforce load balancing</li> <li>The gating function requires to sample a gaussian and to train another set of weights \(W_{noise}\)</li> </ul> <p>In the GShard paper the authors introduce a single auxiliary loss to balance the load that is both simpler and doesn’t require to modify the initial framing of the gating function computation.</p> <p>As explained earlier our unatainable goal is to have a loss function \(\mathcal{L}_{aux}(N_1,...,N_k)\) that is minimized when the \(N_i\) are uniform. While the previous loss was the square of the coeficient of variation of the \(N_i\) another valid loss would be: \(\mathcal{L}_{aux}=\sum_{i=0}^{k}N_i \cdot N_i\)</p> <p>Indeed minimizing this quantity is equivalent to variance of the distribution of the \(N_i\) and thus is minimized for a uniform distribution.</p> <p>What can now be done to make this function differentiable is to replace only one of the \(N_i\) in \(N_i \cdot N_i\) by the proxy \(P_i=\sum_{x \in \mathcal{B}}G_i(x)\) leading to a loss function that is differentiable while still taking into account not only the importance but the real load of each expert.</p> <p>The final loss function can thus be expressed as: \(\mathcal{L}_{aux}=\sum_{i=0}^{k}N_i \cdot P_i\)</p> <h3 id="wang-2024-auxilliary-loss-free-load-balancing"><a class="citation" href="#loss_free_balancing">(Wang, 2024)</a>: Auxilliary loss free load balancing</h3> <p>While improving the load balancing an auxiliary loss interfere with the training process and may decrease the true model performance. Indeed, the gradients coming from this loss get back propagated to \(H(x) = W_g \cdot x\) modifying \(W_g\) but also affect the weights of all the layers prior to the gating function. Such a behavior is not desired and is fixed by this paper from DeepSeek.</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mixture_of_experts/auxiliary_loss_free-480.webp 480w,/assets/img/mixture_of_experts/auxiliary_loss_free-800.webp 800w,/assets/img/mixture_of_experts/auxiliary_loss_free-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mixture_of_experts/auxiliary_loss_free.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Illustration of the dilemma between performance and load balancing with loss free balancing breaking this dilemma. The max violation being defined as $$\text{MaxVio} = \frac{\max_i \text{Load}_i - \overline{\text{Load}_i}}{\overline{\text{Load}_i}}$$ (Image source: <a class="citation" href="#loss_free_balancing">(Wang, 2024)</a>) </div> <p>The general idea of this method is very simple: We can add biases to the computation of the gating functions and update directly(without backpropagation) these bias to achieve loss balancing.</p> <p>More formally the function \(H(x)\) is defined as: \(H(x)=W_g \cdot x + b\) For each training batch we update the biases by counting for each expert the number of token they are assigned \(c_i\) and the the number of tokens they should receive if the gating was uniform \(\bar{c_i}\). We can simply update as \(b_i^{new}=b_i+u*sign(c_i-\bar{c_i})\).</p> <p>TODO: References sans parenthese qui link vers l’URL et avec le nom qui faut Finir calcul + plot pour EM Finir introduction de use of MoE in LLM (parler de conditional computation etc… (regarder huggingface)) DeepseekMoE</p> <h1 id="references">References</h1> <ol class="bibliography"><li><span id="mixture_density_network">Bishop. (2024). <i>Mixture Density Networks</i>. https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf</span></li> <li><span id="articulatory_inversion">Uria, et al. (2012). <i>Deep Architectures for Articulatory Inversion</i>. https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf</span></li> <li><span id="mixtral_of_experts">Jiang, et al. (2024). <i>Mixtral Of Experts</i>. https://arxiv.org/pdf/2401.04088</span></li> <li><span id="sparse_moe">Shazeer, et al. (2017). <i>Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</i>. https://arxiv.org/pdf/1701.06538</span></li> <li><span id="gshard">Lepikhin, et al. (2020). <i>GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding</i>. https://arxiv.org/pdf/2006.16668</span></li> <li><span id="loss_free_balancing">Wang, et al. (2024). <i>Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts</i>. https://arxiv.org/pdf/2408.15664</span></li></ol>]]></content><author><name></name></author><category term="llm"/><summary type="html"><![CDATA[Fondations of Mixture of Experts (MoE)]]></summary></entry></feed>