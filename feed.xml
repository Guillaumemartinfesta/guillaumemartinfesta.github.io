<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://guillaumemartinfesta.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://guillaumemartinfesta.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-02-10T10:50:16+00:00</updated><id>https://guillaumemartinfesta.github.io/feed.xml</id><title type="html">blank</title><entry><title type="html">My Template Post</title><link href="https://guillaumemartinfesta.github.io/blog/2025/my-template-post/" rel="alternate" type="text/html" title="My Template Post"/><published>2025-02-03T00:00:00+00:00</published><updated>2025-02-03T00:00:00+00:00</updated><id>https://guillaumemartinfesta.github.io/blog/2025/my-template-post</id><content type="html" xml:base="https://guillaumemartinfesta.github.io/blog/2025/my-template-post/"><![CDATA[<p>layout: post title: Mixture of Experts: From the ground up to use in LLMs. date: 2024-10-02 description: . tags: formatting links categories: sample-posts —</p> <p>I/ Fondations of Mixture of Experts (MoE)</p> <p>1) Theoretical Basis</p> <p>Mixture of Experts are a special case of Latent (models such as Mixture of Gaussians) meaning that in order to fit a given distribution they introduce and rely on hidden variables.</p> <p>We will here give the guiding example we will refer to in this example:</p> <p>Let’s say we want to fit this distribution:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mixture_of_experts/output-480.webp 480w,/assets/img/mixture_of_experts/output-800.webp 800w,/assets/img/mixture_of_experts/output-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mixture_of_experts/output.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>II/ Use of MoEs for LLMs</p>]]></content><author><name></name></author><summary type="html"><![CDATA[layout: post title: Mixture of Experts: From the ground up to use in LLMs. date: 2024-10-02 description: . tags: formatting links categories: sample-posts —]]></summary></entry></feed>