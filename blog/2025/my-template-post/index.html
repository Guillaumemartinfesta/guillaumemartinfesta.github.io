<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>layout: post title: Mixture of Experts: From the ground up to use in LLMs. date: 2024-10-02 description: . tags: formatting links categories: sample-posts —</p> <p>I/ Fondations of Mixture of Experts (MoE)</p> <p>1) Theoretical Basis</p> <p>Mixture of Experts are a special case of Latent (models such as Mixture of Gaussians) meaning that in order to fit a given distribution they introduce and rely on hidden variables.</p> <p>We will here give the guiding example we will refer to in this example:</p> <p>Let’s say we want to fit this distribution:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mixture_of_experts/output-480.webp 480w,/assets/img/mixture_of_experts/output-800.webp 800w,/assets/img/mixture_of_experts/output-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/mixture_of_experts/output.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>II/ Use of MoEs for LLMs</p> </body></html>