<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h1 id="i-fondations-of-mixture-of-experts-moe">I/ Fondations of Mixture of Experts (MoE)</h1> <h2 id="1-problem-description">1) Problem Description</h2> <p>Let’s say we want to fit this distribution of y given x:</p> <p>We first introduce the direct problem:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mixture_of_experts/direct_problem-480.webp 480w,/assets/img/mixture_of_experts/direct_problem-800.webp 800w,/assets/img/mixture_of_experts/direct_problem-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/mixture_of_experts/direct_problem.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>As we will see shortly after we swaped the x and y axis as the problem of interest will actually be the inverse of this one. We can see here that predicting x given y corresponds to predicting a trend x=f(y) and accounting for some measuring noise. As the trend is a function mapping from y to x (with only one x associated to a given y) the distribution we are trying to predict is unimodal, a sensible model to fit to this data would thus have this form:</p> <p>$y \sim \mathcal{N}(\mu(x),\sigma^2)$</p> <p>The parameters to fit for this model are the parameters $\theta$ of $\mu(x)$ estimating the mode/trend and $\sigma^2$ accounting for the residual noise. $\mu(x)$ could be any sort of model: linear regression, polynomial regression, kernel based model, neural network…</p> <p>This problem description is very classic and enables to simply fit by trying to minimize the NLL(Negative Log Likelihood).</p> <p>However considering the inverse problem by predicting y as a function of x does not lead to such nice properties:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mixture_of_experts/inverse_problem-480.webp 480w,/assets/img/mixture_of_experts/inverse_problem-800.webp 800w,/assets/img/mixture_of_experts/inverse_problem-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/mixture_of_experts/inverse_problem.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>While the direct problem led to a simple unimodal distribution, we see that the inverse problem has 2 modes for x between 0.4 and 0.6 making the previous model obsolete. We see that such a phenomenon of a multimodal inverse problem arises whenever we consider the inverse of a function that is not injective.</p> <p>Luckily Mixture of Experts(MoE) is a type of models that enables to solve this issue. MoEs belong to the class Latent Variable Models(LVM) that rely on some hidden variables (the latent variables) to make predictions.</p> <table> <tbody> <tr> <td>More formally we could introduce a categorical latent variable z such that $p(z</td> <td>x)=Cat(z</td> <td>Softmax(V^T(1,x)))$ that will determine the “expert” to use.The function p(z</td> <td>x) is called the gating function. Each of the N experts can then be a model of the form described earlier $p(y</td> <td>x,z=k)=\mathcal{N}(y</td> <td>\mu_k(x),\sigma_k^2)$ describing an unimodal distribution. As for a given x we could have a nonzero probability of “activating” each expert, we can then describe any distribution having less than N modes.</td> </tr> </tbody> </table> <p>The model parameters are then $\theta_1,…,\theta_k$ and $V \in \mathcal{M}_{2 \times N}$. We denote by $\theta$ all these parameters.</p> <h2 id="2-fitting-moes-the-em-algorithm">2) Fitting MoEs: the EM algorithm</h2> <table> <tbody> <tr> <td>Fitting a Latent Variable Model is not straight forward. The joint log probability $log(p(y,z</td> <td>x,\theta))$ is easy to compute but the observed data log likelihood is hard to compute since $log(p(y</td> <td>x,\theta))=log(\sum p(y,z</td> <td>x,theta))$.</td> </tr> </tbody> </table> <p>The log can’t be pushed into the sum. Moreover most of the simple models lead to a convex log likelihood but this property is lost for LVM as the logarithm of the sum of 2 log convex functions is not necessarly convex making the optimization problem much harder.</p> <table> <tbody> <tr> <td>As the complete data log probability is easy to compute we estimate the observed data log likelihood log(p(x)) by the expected complete data log likelihood $\mathbb{E}_{z \sim q(z</td> <td>x,y)}[log(P(y,z</td> <td>x))]$</td> </tr> </tbody> </table> <p>The EM algorithm does exactly that in order by alternating between 2 steps:</p> <ul> <li> <table> <tbody> <tr> <td>The E (Estimation) step where we estimate $q(z_i</td> <td>x_i,y_i)=p(z_i=k</td> <td>x_i,y_i,\hat{\theta}^t)$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>The M (Maximization) step were we compute $\hat{\theta}^{t+1}=\underset{\theta}{argmax}\mathbb{E}_{z \sim q(z</td> <td>x,y)}[log(P(y,z</td> <td>x))]$</td> </tr> </tbody> </table> </li> </ul> <table> <tbody> <tr> <td>In other words the EM algorithm iteratively computes $\hat{\theta}^{t+1}=\underset{\theta}{argmax}\sum_{i=1}^n\sum_{k=1}^Nlog[p(y_i,z_i=k</td> <td>x_i,\theta)]p(z_i=k</td> <td>x_i,\hat{\theta^t})$</td> </tr> </tbody> </table> <h2 id="3-theoretical-proofs-for-the-em-algorithm">3) Theoretical proofs for the EM algorithm</h2> <p>We will show in this part that the EM algorithm monotically increases the observed data log likelihood.</p> <p>Our first goal is to derive a formula showing how far of our estimation of the observed data log likelihood by the expected complete data log likelihood is.</p> <p>To make the point more general we assume the variable z is continuous (the same results also hold for discrete z)</p> <p>[ \begin{align} \mathbb{E}<em>{z \sim q(z|x,y)}[log(p(y,z|x,\theta))] <br> = \int_zq(z|x,y)log(p(y,z|x,\theta))dz <br> = \int_zq(z|x,y) \left[ log(\frac{p(y,z|x,\theta)}{q(z|x,y)})+log(q(z|x,y))\right]dz <br> =\int_zq(z|x,y)log(\frac{p(z|x,y,\theta)p(y|x,\theta)}{q(z|x,y)})dz + \mathbb{H}_q <br> =\int_zq(z|x,y)log(p(y|x,\theta))dz - \int_zq(z|x,y)log(\frac{q(z|x,y)}{p(z|x,y,\theta)})dz + \mathbb{H}_q <br> =log(p(y|x,\theta))+D</em>{KL}(q(.|x,y)||p(.|x,y,\theta))+\mathbb{H}_q \end{align} ]</p> <h1 id="ii-use-of-moes-for-llms">II/ Use of MoEs for LLMs</h1> </body></html>