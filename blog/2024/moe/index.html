<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script async src="https://www.googletagmanager.com/gtag/js?id=G-SDHSP0G785"></script> <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-SDHSP0G785');
    </script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Mixture of Experts: From the ground up to use in LLMs. | Guillaume Martin </title> <meta name="author" content="Guillaume Martin"> <meta name="description" content=""> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://guillaumemartinfesta.github.io/blog/2024/moe/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Guillaume</span> Martin </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Mixture of Experts: From the ground up to use in LLMs.</h1> <p class="post-meta"> Created in October 02, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/category/llm"> <i class="fa-solid fa-tag fa-sm"></i> llm</a> </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h1"> <a href="#foundations-of-mixture-of-experts-moe">Foundations of Mixture of Experts (MoE)</a> <ul> <li class="toc-entry toc-h2"><a href="#problem-description">Problem Description</a></li> <li class="toc-entry toc-h2"><a href="#fitting-moes-the-em-algorithm">Fitting MoEs: the EM algorithm</a></li> <li class="toc-entry toc-h2"><a href="#application-on-our-specific-problem">Application on our specific problem</a></li> <li class="toc-entry toc-h2"><a href="#theoretical-proofs-for-the-em-algorithm">Theoretical proofs for the EM algorithm</a></li> </ul> </li> <li class="toc-entry toc-h1"> <a href="#use-of-moes-in-deep-learning-and-llms">Use of MoEs in Deep Learning and LLMs</a> <ul> <li class="toc-entry toc-h2"><a href="#mixture-density-networks-mdn">Mixture Density Networks (MDN)</a></li> <li class="toc-entry toc-h2"><a href="#use-of-moes-in-llm">Use of MoEs in LLM</a></li> <li class="toc-entry toc-h2"><a href="#various-design-choices">Various design choices</a></li> <li class="toc-entry toc-h2"> <a href="#load-balancing-strategies">Load balancing strategies</a> <ul> <li class="toc-entry toc-h3"><a href="#shazeer-2017-2-differentiable-auxiliary-loss-terms">(Shazeer, 2017): 2 differentiable auxiliary loss terms</a></li> <li class="toc-entry toc-h3"><a href="#lepikhin-2020-a-single-unified-auxiliary-loss">(Lepikhin, 2020): A single unified auxiliary loss</a></li> <li class="toc-entry toc-h3"><a href="#wang-2024-auxiliary-loss-free-load-balancing">(Wang, 2024): Auxiliary loss-free load balancing</a></li> </ul> </li> </ul> </li> <li class="toc-entry toc-h1"> <a href="#efficient-implementations-of-mixture-of-experts">Efficient implementations of Mixture of Experts</a> <ul> <li class="toc-entry toc-h2"> <a href="#computation-on-a-single-device">Computation on a single device</a> <ul> <li class="toc-entry toc-h3"><a href="#batched-matrix-multiplication">Batched Matrix Multiplication</a></li> <li class="toc-entry toc-h3"> <a href="#block-sparsity-primitives">Block sparsity primitives</a> <ul> <li class="toc-entry toc-h4"><a href="#why-do-we-need-to-have-block-sparsity-and-not-sparsity">Why do we need to have block sparsity and not sparsity?</a></li> <li class="toc-entry toc-h4"><a href="#why-do-we-still-need-to-permute-and-unpermute-the-tokens">Why do we still need to permute and unpermute the tokens?</a></li> </ul> </li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#distributed-moes">Distributed MoEs</a> <ul> <li class="toc-entry toc-h3"><a href="#data-parallelism">Data Parallelism</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#model-parallelism">Model Parallelism</a> <ul> <li class="toc-entry toc-h3"><a href="#expert-parallelism">Expert Parallelism</a></li> <li class="toc-entry toc-h3"><a href="#expert--model--data-parallelism">Expert + Model + Data Parallelism</a></li> </ul> </li> </ul> </li> <li class="toc-entry toc-h1"><a href="#references">References</a></li> </ul> </div> <hr> <div id="markdown-content"> <h1 id="foundations-of-mixture-of-experts-moe">Foundations of Mixture of Experts (MoE)</h1> <p>This section is inspired by Chapter 11 of the amazing book Machine Learning: A Probabilistic Perspective by Kevin P. Murphy.</p> <h2 id="problem-description">Problem Description</h2> <p>Let’s say we want to fit this distribution of y given x:</p> <p>We first introduce the direct problem:</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mixture_of_experts/direct_problem-480.webp 480w,/assets/img/mixture_of_experts/direct_problem-800.webp 800w,/assets/img/mixture_of_experts/direct_problem-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/mixture_of_experts/direct_problem.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Direct problem distribution </div> <p>As we will see shortly after we swapped the x and y axis as the problem of interest will actually be the inverse of this one. We can see here that predicting x given y corresponds to predicting a trend x=f(y) and accounting for some measuring noise. As the trend is a function mapping from y to x (with only one x associated with a given y), the distribution we are trying to predict is unimodal. A sensible model to fit to this data would thus have this form:</p> \[y \sim \mathcal{N}(\mu(x \mid \theta),\sigma^2)\] <p>The parameters to fit for this model are the parameters \(\theta\) of \(\mu(x \mid \theta)\) estimating the mode/trend and \(\sigma^2\) accounting for the residual noise. \(\mu(x \mid \theta)\) could be any sort of model: linear regression, polynomial regression, kernel-based model, neural network…</p> <p>This problem description is very classic and enables to simply fit by trying to minimize the NLL (Negative Log Likelihood).</p> <p>However, considering the inverse problem by predicting y as a function of x does not lead to such nice properties:</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mixture_of_experts/inverse_problem-480.webp 480w,/assets/img/mixture_of_experts/inverse_problem-800.webp 800w,/assets/img/mixture_of_experts/inverse_problem-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/mixture_of_experts/inverse_problem.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Inverse problem distribution </div> <p>While the direct problem led to a simple unimodal distribution, we see that the inverse problem has 2 modes for x between 0.4 and 0.6, making the previous model obsolete. We see that such a phenomenon of a multimodal inverse problem arises whenever we consider the inverse of a function that is not injective.</p> <p>Luckily, Mixture of Experts (MoE) is a type of model that enables to solve this issue. MoEs belong to the class Latent Variable Models (LVM) that rely on some hidden variables (the latent variables) to make predictions.</p> <p>More formally, we could introduce a categorical latent variable z such that \(p(z\mid x)=Cat(z\mid Softmax(V^T(1,x)))\) that will determine the “expert” to use. The function \(p(z\mid x)\) is called the gating function. Each of the N experts can then be a model of the form described earlier \(p(y\mid x,z=k)=\mathcal{N}(y\mid\mu_k(x \mid \theta_k),\sigma_k^2)\) describing an unimodal distribution. As for a given x we could have a nonzero probability of “activating” each expert, we can then describe any distribution having less than N modes.</p> <p>The model parameters are then \(\theta_1,...,\theta_k,\sigma_1^2,...,\sigma_k^2\) and \(V \in \mathcal{M}_{2 \times N}\). We denote by \(\theta\) all these parameters.</p> <h2 id="fitting-moes-the-em-algorithm">Fitting MoEs: the EM algorithm</h2> <p>Fitting a Latent Variable Model is not straightforward. The joint log probability \(log(p(y,z\mid x,\theta))\) is easy to compute but the observed data log likelihood is hard to compute since \(log(p(y\mid x,\theta))=log(\sum p(y,z\mid x,theta))\).</p> <p>The log can’t be pushed into the sum. Moreover, most of the simple models lead to a convex log likelihood but this property is lost for LVM as the logarithm of the sum of 2 log convex functions is not necessarily convex, making the optimization problem much harder.</p> <p>As the complete data log probability is easy to compute, we estimate the observed data log likelihood log(p(x)) by the expected complete data log likelihood \(\mathbb{E}_{z \sim q(z\mid x,y)}[log(P(y,z\mid x))]\)</p> <p>The EM algorithm does exactly that in order by alternating between 2 steps:</p> <ul> <li> <p>The E (Estimation) step where we estimate \(q^t(z_i\mid x_i,y_i)=p(z_i=k\mid x_i,y_i,\hat{\theta}^t)\)</p> </li> <li> <p>The M (Maximization) step where we compute \(\hat{\theta}^{t+1}=\underset{\theta}{argmax}\sum_{i=1}^n\mathbb{E}_{z_i \sim q^t(z_i\mid x_i,y_i)}[log(P(y_i,z_i\mid x_i))]\)</p> </li> </ul> <p>In other words, the EM algorithm iteratively computes</p> \[\begin{align*} &amp;\hat{\theta}^{t+1}=\underset{\theta}{argmax}Q(\theta,\hat{\theta}^{t}) \\ &amp;Q(\theta,\hat{\theta}^{t})=\sum_{i=1}^n\sum_{k=1}^Nlog[p(y_i,z_i=k\mid x_i,\theta)]p(z_i=k\mid x_i,\hat{\theta^t}) \end{align*}\] <p>This method can be used whenever we can approximate the posterior probability in the E step. It is however also necessary to be able to fit rapidly the expert models (for example by having closed form formulas or a convex NLL) as we will have to fit the N experts for every iteration.</p> <h2 id="application-on-our-specific-problem">Application on our specific problem</h2> <p>For this specific problem, we use 3 experts which are all linear regressions:</p> \[\begin{align*} &amp;p(z\mid x)=Cat(z\mid Softmax(V^T(1,x))) \\ &amp;p(y\mid x,z=k)=\mathcal{N}(y\mid \omega_k^Tx,\sigma_k^2) \end{align*}\] <p>For a given time step t, we can compute the E step using Bayes rule:</p> \[r_{ik}=p(z_i=k \mid x_i,y_i)=\frac{p(y_i \mid x_i,z_i=k)p(z_i=k \mid x)}{\sum_{k=1}^3p(y_i \mid x_i,z_i=k)p(z_i=k \mid x)}\] <p>For the M step, the expected complete data log likelihood is given by:</p> \[\begin{align*} Q(\theta, \theta^{old}) &amp;= \sum_{i=1}^{N} \sum_{k=1}^{3} r_{ik} \mathcal{N}(y_i | \mathbf{w}_k^T \mathbf{x}_i, \sigma_k^2) \\ &amp;= \sum_{i=1}^{N}\sum_{k=1}^{3}r_{ik}\log[\mathcal{S}(\mathbf{V}^T \mathbf{x}_i)_k] + \sum_{k=1}^{3}\sum_{i=1}^{N}r_{ik}(-\frac{1}{\sigma_k^2}(y_i-w_k^Tx_i)^2) \end{align*}\] <p>We can recognize that the terms involving \(\mathbf{V}\) correspond to -1 times the soft cross entropy loss with soft labels \(r_{ik}\). Hence, determining \(\mathbf{V}\) simply corresponds to fitting a multiclass logistic regression with soft labels.</p> <p>For each k, the terms involving \(\omega_k\) and \(\sigma_k^2\) correspond to:</p> \[\sum_{i=1}^{N} r_{ik} \left\{ -\frac{1}{\sigma_k^2} (y_i - \mathbf{w}_k^T \mathbf{x}_i)^2 \right\}\] <p>The objective function for each \(w_k\) can then be recognized as a weighted least square problem which can also be optimized efficiently through linear algebra or convex optimization.</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mixture_of_experts/linear_regression_mixture_of_experts-480.webp 480w,/assets/img/mixture_of_experts/linear_regression_mixture_of_experts-800.webp 800w,/assets/img/mixture_of_experts/linear_regression_mixture_of_experts-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/mixture_of_experts/linear_regression_mixture_of_experts.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mixture_of_experts/gating_function_mixture_of_experts-480.webp 480w,/assets/img/mixture_of_experts/gating_function_mixture_of_experts-800.webp 800w,/assets/img/mixture_of_experts/gating_function_mixture_of_experts-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/mixture_of_experts/gating_function_mixture_of_experts.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Mixture of Experts model fitted to our inverse problem. </div> <p>We can see once again the power of the EM algorithm that incrementally optimizes a highly non-convex objective while having simple steps optimizing convex objectives.</p> <h2 id="theoretical-proofs-for-the-em-algorithm">Theoretical proofs for the EM algorithm</h2> <p>We will show in this part that the EM algorithm monotonically increases the observed data log likelihood.</p> <p>Our first goal is to derive a formula showing how far our estimation of the observed data log likelihood by the expected complete data log likelihood is.</p> <p>To make the point more general, we assume the variable z is continuous (the same results also hold for discrete z)</p> \[\begin{align*} L_{\theta,q}(x,y)=\mathbb{E}_{z \sim q(z\mid x,y)}[\log(p(y,z\mid x,\theta))] &amp;= \int_z q(z\mid x,y) \log(p(y,z\mid x,\theta)) \, dz \\ &amp;= \int_z q(z\mid x,y) \left[ \log\left(\frac{p(y,z\mid x,\theta)}{q(z\mid x,y)}\right) + \log(q(z\mid x,y)) \right] dz \\ &amp;= \int_z q(z\mid x,y) \log\left(\frac{p(z\mid x,y,\theta) p(y\mid x,\theta)}{q(z\mid x,y)}\right) dz + \mathbb{H}_q \\ &amp;= \int_z q(z\mid x,y) \log(p(y\mid x,\theta)) \, dz - \int_z q(z\mid x,y) \log\left(\frac{q(z\mid x,y)}{p(z\mid x,y,\theta)}\right) dz + \mathbb{H}_q \\ &amp;= \log(p(y\mid x,\theta)) - D_{KL}(q(z\mid x,y) \mid \mid p(z\mid x,y,\theta)) + \mathbb{H}_q \end{align*}\] <p>As the term \(\mathbb{H}_q\) doesn’t depend on \(\theta\), we have:</p> \[\begin{align*} &amp;\underset{\theta}{argmax}L_{\theta,q}(x,y)=\underset{\theta}{argmax}\mathcal{L} \\ &amp;\mathcal{L}(\theta,q)=L_{\theta,q}(x,y)-\mathbb{H}_q=\log(p(y\mid x,\theta)) - D_{KL}(q(z\mid x,y) \mid \mid p(z\mid x,y,\theta)) \end{align*}\] <p>We thus have that \(\mathcal{L}\), also called the evidence lower bound or ELBO, is a lower bound of the observed data log likelihood.</p> <p>At time step t, we take \(q^t(z_i\mid x_i,y_i)=p(z_i=k\mid x_i,y_i,\hat{\theta}^t)\), thus leading to the KL divergence term being 0 for \(\theta=\hat{\theta}^t\). We note \(\mathcal{L}(\theta,\hat{\theta}^t)=\mathcal{L}(\theta,q^t)\) and thus have \(\mathcal{L}(\hat{\theta}^t,\hat{\theta}^t)=log(p(y \mid x, \hat{\theta}^t))\).</p> <p>This property of having a lower bound that is tight around \(\hat{\theta}^t\) is the one guaranteeing the increase in the log likelihood as we have:</p> <p>\(\begin{align*} log(p(y|x,\hat{\theta}^{t+1})) &amp;\geq \mathcal{L}(\hat{\theta}^{t+1},\hat{\theta}^{t}) \\ &amp;=\underset{\theta}{max}\mathcal{L}(\theta,\hat{\theta}^{t}) \\ &amp;\geq \mathcal{L}(\hat{\theta}^{t},\hat{\theta}^{t}) \\ &amp;=log(p(y|x,\hat{\theta}^{t})) \end{align*}\).</p> <p>The EM is thus a part of the MM (Minoration Maximization) class of algorithms, aiming at maximizing a tight minoration of a given objective function. Other important ML algorithms like TRPO (Trust Region Policy Optimization) in the domain of RL rely on this class of methods.</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mixture_of_experts/MM_illustration-480.webp 480w,/assets/img/mixture_of_experts/MM_illustration-800.webp 800w,/assets/img/mixture_of_experts/MM_illustration-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/mixture_of_experts/MM_illustration.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Illustration of EM, we approximate our non-convex NLL by a lower bound that is tight around $\hat{\theta}^t$ and that is easy to optimize (convex if the experts' NLL is convex). We see graphically that this can only lead to an increase in the log likelihood. </div> <h1 id="use-of-moes-in-deep-learning-and-llms">Use of MoEs in Deep Learning and LLMs</h1> <h2 id="mixture-density-networks-mdn">Mixture Density Networks (MDN)</h2> <p>In the case of neural networks, using the EM algorithm might not be the most appropriate choice for several reasons:</p> <ul> <li>Training a neural network is costly, using EM would lead to \(N_{experts} \times N_{EM\_iterations}\) training procedures introducing a huge overhead.</li> <li>One of the strengths of EM is that it keeps the convexity properties of the expert models for the M step. The loss landscape of Neural Networks being non-convex, this property isn’t of any interest.</li> </ul> <p>What has instead been proposed in <a class="citation" href="#mixture_density_network">(Bishop, 2024)</a> is to estimate the gating functions using a Neural Network. This approach is of particular interest given the structure of NN as we can have the first part of the model being shared to then add specific heads for each expert and for the gating functions. Such an approach enables to “factorize” computations and to perform inference in one forward pass.</p> <p>The parameters can then be updated using backpropagation using the negative log likelihood as a loss \(log(p(y \mid x))=log\left(\sum_{k=1}^{N}p(y \mid z=k, x)p(z=k \mid x)\right)\).</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mixture_of_experts/MDN-480.webp 480w,/assets/img/mixture_of_experts/MDN-800.webp 800w,/assets/img/mixture_of_experts/MDN-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/mixture_of_experts/MDN.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Illustration of a MDN, the $$\alpha$$ coefficients corresponding to the estimates of p(z=k|x). (Image source: <a class="citation" href="#articulatory_inversion">(Uria, 2012)</a>) </div> <h2 id="use-of-moes-in-llm">Use of MoEs in LLM</h2> <p>The previous framing of Mixture of Experts was purely probabilistic and was designed to fit a given probability distribution. Such a notion can however be generalized by seeing MoEs as taking the average of the outputs of each expert \(E_k(x)\) weighted by the gating functions \(G(x)=(p(z=k \mid x))_k\). We thus have more generally a way to combine the outputs of various models \(f(x)=\sum_{k=1}^NE_k(x)G(x)_k\).</p> <p>In the case of Neural Networks, we can go further and recognize that a Mixture of Experts does not need to be considered only at the model level but actually at the layer level. MoE layers can then be interleaved with other blocks (MLP block, Attention block…) and/or stacked one after the other.</p> <p>We can even use Mixture of Experts as the experts of an MoE layer leading to hierarchical mixture of experts.</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mixture_of_experts/sparse_mixture_of_experts_mistral-480.webp 480w,/assets/img/mixture_of_experts/sparse_mixture_of_experts_mistral-800.webp 800w,/assets/img/mixture_of_experts/sparse_mixture_of_experts_mistral-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/mixture_of_experts/sparse_mixture_of_experts_mistral.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Illustration of a mixture of experts layer. We call "router" the part of the model that determines the gating functions. (Image source: <a class="citation" href="#mixtral_of_experts">(Jiang, 2024)</a>) </div> <p>In the previous section, MoEs were particularly useful as they led to a global model that is more expressive than any of its components (enabling to represent a multimodal distribution using unimodal components). In the case of LLMs, the model already being very expressive (MLPs being universal function approximators), this is not the most useful property.</p> <p>Mixture of Experts can however turn out to be very useful if we add sparsity constraints to the gating functions. The output of a MoE module being the average of the experts weighted by the gating functions, we have that experts whose gating function is small contribute little to nothing to the final output. We can thus approximate the output by setting the smallest gating functions to zero, thus avoiding the computation of the corresponding expert outputs. This possibility of dynamically choosing which computation to perform is called conditional computation. The total number of parameters is thus not the same as the number of active parameters during inference.</p> <p>One common method is to define the final gating function as:</p> \[\begin{align*} &amp;G(x)=Softmax(KeepTopk(H(x),K)) \\ &amp;H(x)=W_g \cdot x \\ &amp;(KeepTopk(v),K)_i= \begin{cases} v_{i}, &amp; \text{if } s_{i,t} \in \text{Topk}(\{v_{i} \mid 1 \leq i \leq N\}, K) \\ -\infty, &amp; \text{otherwise} \end{cases} \end{align*}\] <p>The scaling laws <a class="citation" href="#scaling_laws">(Kaplan, 2020)</a> tell us that the more parameters a model has, the better it performs. Scaling up the number of parameters, however, leads to a roughly quadratic increase in training compute (linear increase in the computation per forward pass and linear increase in the data to train on). Mixture of Experts enables to scale up the total number of parameters a model has with a subquadratic increase in computation, making them a promising prospect. This is the very reason some of the best LLMs like GPT4, Mixtral_8x7B, or more recently Deepseek V3 have leveraged this architecture.</p> <p>In practice, mixture of experts are used to replace all the Feed Forward Networks (FFN = MLP with 2 layers) in LLMs but are not used at all for the attention layers.</p> <h2 id="various-design-choices">Various design choices</h2> <p>Various models developed throughout the years have developed with different design choices:</p> <table> <thead> <tr> <th>Model</th> <th>Number of Experts (N)</th> <th>Top-K Routing</th> <th>Total Parameters</th> <th>Active Parameters</th> </tr> </thead> <tbody> <tr> <td>Switch Transformer</td> <td>128</td> <td>1</td> <td>26B</td> <td>700M</td> </tr> <tr> <td>GShard</td> <td>16</td> <td>2</td> <td>136B</td> <td>21.6B</td> </tr> <tr> <td>Mixtral 8x7B</td> <td>8</td> <td>2</td> <td>47B</td> <td>13B</td> </tr> </tbody> </table> <p>More recently, DeepSeek introduced DeepseekMoE <a class="citation" href="#deepseekmoe">(Dai, 2024)</a> which aims at enabling experts to be more specialized (the very reason of mixture of experts) with two distinct modifications:</p> <ul> <li> <p>Fine-grained experts: In order to have more specialized experts, the authors suggest scaling both N and K by the same multiplicative factor m. And to multiply the inner dimension of the FFN by \(\frac{1}{m}\) to keep the same overall computational cost. The idea is that by splitting each expert into m smaller experts as this method does, we encourage to have experts that are more highly specialized.</p> </li> <li> <p>Shared Expert Isolation: A fixed number of these experts, denoted as the shared experts, will always be gated to. The goal of these experts is to apply any of the common knowledge/information required by the tokens. The idea is that without these shared experts, some different experts will have to acquire this general information leading to redundancies. Using shared experts should lead to less redundancy in the rest of the experts.</p> </li> </ul> <p>With Deepseek MoE has 144.6B total parameters but only 22.2B active parameters.</p> <h2 id="load-balancing-strategies">Load balancing strategies</h2> <p>During the training, the naive MoE architecture might tend to only route responses to a restricted subset of experts leading to a reduction of performance (as we store in memory the weights of the other experts that are effectively useless as they are never called). Such a phenomenon, called routing collapse, comes from the fact that if the routing is focused on an expert at the beginning of the training, this expert will receive more training samples and thus train faster than the other experts, pushing the router to allocate even more resources to this expert.</p> <p>In order to fix this problem, several strategies have been proposed:</p> <h3 id="shazeer-2017-2-differentiable-auxiliary-loss-terms"> <a class="citation" href="#sparse_moe">(Shazeer, 2017)</a>: 2 differentiable auxiliary loss terms</h3> <p>The goal of the auxiliary loss should be to have a uniform repartition of the “activation” of each expert. A naive idea would be to define a loss on a batch of size N taking \(\mathcal{L}_{aux}(N_1,...,N_k)\) with \(N_i=\sum_{x \in \mathcal{B}} \mathbf{1}(\text{x is assigned to Expert i})\) the number of examples in the batch getting routed to the i-th expert.</p> <p>The loss could then be any form of loss pushing the \(N_i\) to be uniform. For example, such a loss could be the square of the coefficient of variation of \((N_i)_i\):</p> \[\mathcal{L}_{aux}(N_1,...,N_k)=\lambda_{1}CV((N_i)_i))^2=(\frac{\sigma_N}{\mu_N})^2\] <p>\(\lambda_{1}\) being the penalization coefficient.</p> <p>However, this approach cannot work as the \(N_i\) are non-differentiable and thus can’t be optimized through standard gradient descent.</p> <p>To resolve this issue, <a class="citation" href="#sparse_moe">(Shazeer, 2017)</a> introduces 2 loss terms:</p> <p>The first one looks like the loss \(\mathcal{L}_{aux}\) we defined earlier but instead of taking the \(N_i\) it takes in the \(P_i=\sum_{x \in \mathcal{B}}G_i(x)\) (called the importance of expert i) as a proxy. Such a proxy helps balancing the load but is far from perfect as we could have a uniform importance among experts while having an imbalanced gating: experts “activated” rarely with high gating weight can have the same importance as experts “activated” frequently with low gating weight. A second auxiliary loss term is thus added in order to complement the 1st one.</p> <p>In order to introduce the 2nd loss term, we first need to see the modifications to the gating functions made by the paper:</p> <p>The function \(H(x)\) which was previously defined as \(W_g \cdot x\) is now tweaked to have this form:</p> \[H(x)_i = (W_g \cdot x)_i + \text{StandardNormal}() \cdot \text{Softplus}(W_{\text{noise}}\cdot x)_i\] <p>with \(Softplus(x)=ln(1+e^x)\) a smooth approximation of the ReLU.</p> <p>The general idea is that by making the gating process stochastic, we can consider the differentiable smooth probability of an expert being chosen rather than the non-differentiable and discrete event of actually being chosen. We also introduce a learned scaling factor \(W_{noise}\) for the normal distribution.</p> <p>We can then define a smooth proxy for \(N_i\) as:</p> \[\begin{align*} &amp;Load_i=\sum_{x \in \mathcal{B}}(P_{expert})_i \\ &amp;(P_{expert})_i = \Phi\left(\frac{(x \cdot W_g)_i - \text{kth\_excluding}(H(x), k, i)}{\text{Softplus}((x \cdot W_{\text{noise}})_i)}\right) \end{align*}\] <p>With \(\Phi\) the CDF (Cumulative Distribution Function) of a Gaussian.</p> <p>Finally, the 2nd loss term can then be defined as \(\lambda_{2} \cdot CV((Load_i)_i)^2\).</p> <h3 id="lepikhin-2020-a-single-unified-auxiliary-loss"> <a class="citation" href="#gshard">(Lepikhin, 2020)</a>: A single unified auxiliary loss</h3> <p>The previous approach suffers from a few disadvantages:</p> <ul> <li>It has 2 distinct auxiliary loss trying to enforce load balancing.</li> <li>The gating function requires to sample a Gaussian and to train another set of weights \(W_{noise}\).</li> </ul> <p>In the GShard paper, the authors introduce a single auxiliary loss to balance the load that is both simpler and doesn’t require to modify the initial framing of the gating function computation.</p> <p>As explained earlier, our unattainable goal is to have a loss function \(\mathcal{L}_{aux}(N_1,...,N_k)\) that is minimized when the \(N_i\) are uniform. While the previous loss was the square of the coefficient of variation of the \(N_i\), another valid loss would be:</p> \[\mathcal{L}_{aux}=\sum_{i=0}^{k}N_i \cdot N_i\] <p>Indeed, minimizing this quantity is equivalent to the variance of the distribution of the \(N_i\) and thus is minimized for a uniform distribution.</p> <p>What can now be done to make this function differentiable is to replace only one of the \(N_i\) in \(N_i \cdot N_i\) by the proxy \(P_i=\sum_{x \in \mathcal{B}}G_i(x)\) leading to a loss function that is differentiable while still taking into account not only the importance but the real load of each expert.</p> <p>The final loss function can thus be expressed as:</p> \[\mathcal{L}_{aux}=\sum_{i=0}^{k}N_i \cdot P_i\] <h3 id="wang-2024-auxiliary-loss-free-load-balancing"> <a class="citation" href="#loss_free_balancing">(Wang, 2024)</a>: Auxiliary loss-free load balancing</h3> <p>While improving the load balancing, an auxiliary loss interferes with the training process and may decrease the true model performance. Indeed, the gradients coming from this loss get backpropagated to \(H(x) = W_g \cdot x\) modifying \(W_g\) but also affect the weights of all the layers prior to the gating function. Such a behavior is not desired and is fixed by this paper from DeepSeek.</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mixture_of_experts/auxiliary_loss_free-480.webp 480w,/assets/img/mixture_of_experts/auxiliary_loss_free-800.webp 800w,/assets/img/mixture_of_experts/auxiliary_loss_free-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/mixture_of_experts/auxiliary_loss_free.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Illustration of the dilemma between performance and load balancing with loss-free balancing breaking this dilemma. The max violation being defined as $$\text{MaxVio} = \frac{\max_i \text{Load}_i - \overline{\text{Load}_i}}{\overline{\text{Load}_i}}$$ (Image source: <a class="citation" href="#loss_free_balancing">(Wang, 2024)</a>) </div> <p>The general idea of this method is very simple: We can add biases to the computation of the gating functions and update directly (without backpropagation) these biases to achieve loss balancing.</p> <p>More formally, the function \(H(x)\) is defined as:</p> \[H(x)=W_g \cdot x + b\] <p>For each training batch, we update the biases by counting for each expert the number of tokens they are assigned \(c_i\) and the number of tokens they should receive if the gating was uniform \(\bar{c_i}\). We can simply update as \(b_i^{new}=b_i+u*sign(c_i-\bar{c_i})\).</p> <h1 id="efficient-implementations-of-mixture-of-experts">Efficient implementations of Mixture of Experts</h1> <p>We saw that mixture of experts can introduce sparsity, thus reducing the effective number of parameters for a forward pass. However, implementing efficient kernels for MoEs is tricky and has been one of the driving forces of the recent adoption of MoEs at scale.</p> <h2 id="computation-on-a-single-device">Computation on a single device</h2> <p>To avoid getting too early into the details of the more complicated distributed implementation of mixture of experts, we will assume for now that all of the computation is done on a single physical device.</p> <h3 id="batched-matrix-multiplication">Batched Matrix Multiplication</h3> <p>The classic approach to compute MoE on hardware is as follows:</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mixture_of_experts/moe_layer_batched_mm-480.webp 480w,/assets/img/mixture_of_experts/moe_layer_batched_mm-800.webp 800w,/assets/img/mixture_of_experts/moe_layer_batched_mm-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/mixture_of_experts/moe_layer_batched_mm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Illustration of the implementation of mixture of experts for $N_{experts}=3$ and $top_k=1$ with token dropping. (Image source: <a class="citation" href="#megablocks">(Gale, 2022)</a>) </div> <p>We first compute the output of the gating function giving us the probabilities of each expert. We then permute the tokens so as to group them by the expert they are assigned to and keep in memory this permutation (in the case where \(top_k&gt;1\) this is not a simple permutation as a token is assigned to multiple experts but can be viewed as a permutation of the tensor where we replicate \(top_k\) times each token). Finally, we compute the output of each expert and unpermute the tokens (In the case where \(top_k&gt;1\) we also compute the average weighted by the gating function outputs for each “duplicated” token).</p> <p>The primitive used to efficiently compute the output of each expert in parallel is the batched matrix multiplication (BMM) that takes a tensor \(M^1\) of shape (b,n,m) and \(M^2\) of shape (b,m,p) and outputs a tensor of shape (b,n,p) where \(output_i=M^1_i@M^2_i\).</p> <p>In this case, the matrix \(M_1\) would store the inputs after permutation with a shape \((N_{experts},capacity, d)\) while \(M_2\) would store the expert weights with shape \((N_{experts},d, d_{ff})\) where \(d_{ff}\) is the width of the middle layer in the FFN (In fact there would be another batched matrix multiply operation with a right operand of shape \((N_{experts},d_{ff},d)\) as a FFN corresponds to a MLP with 2 layers).</p> <p>The curious might have noted that I introduced the capacity in the shape of the first tensor. As the permuted list of tokens should be formatted into a tensor, we need to have the same number of tokens assigned for every expert. This is however not enforced at all during as the routing is dynamic. We need to introduce a quantity that ensures the number of tokens effectively assigned to each expert is fixed.</p> <p>In order to circumvent this issue, the authors of GShard <a class="citation" href="#gshard">(Lepikhin, 2020)</a> introduced the notion of expert capacity that can be computed as follows:</p> \[\text{expert capacity} = \left( \frac{\text{tokens per batch} \times \text{number of routed experts}}{\text{number of experts}} \right) \times \text{capacity factor}.\] <p>The capacity factor is a hyperparameter controlling the “spare room” taken to accommodate an overload of tokens for a given expert. A capacity factor of 1 will make the expert capacity the exact number of tokens each expert should receive under perfectly balanced load. Whenever this load is not completely balanced, there will be token dropping for some overloaded expert and padding for other underloaded experts. While token dropping is not dramatic thanks to the residual connections (the hidden representation of a dropped token will just remain unchanged), token dropping still negatively affects performances and should be avoided.</p> <p>We can see that there is a very subtle dilemma between generating less token dropping &amp; better model performance with a higher capacity factor and reducing the computational overhead with a lower capacity. Such a problem highlights once again how important effective load balancing is for MoEs.</p> <h3 id="block-sparsity-primitives">Block sparsity primitives</h3> <p>To circumvent this issue of token dropping, Megablocks <a class="citation" href="#megablocks">(Gale, 2022)</a> developed new optimized CUDA primitives for blocked sparse matrix multiplication.</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mixture_of_experts/block_sparse_mm-480.webp 480w,/assets/img/mixture_of_experts/block_sparse_mm-800.webp 800w,/assets/img/mixture_of_experts/block_sparse_mm-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/mixture_of_experts/block_sparse_mm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Illustration of the implementation of block sparse matrix multiplication at the right. This enables to have variable number of tokens allocated to experts and even to have experts of different sizes. (Image source: <a class="citation" href="#megablocks">(Gale, 2022)</a>) </div> <p>For simplicity, we will assume that experts have the same size (nearly always the choice made in practice).</p> <p>More formally, we can see that with this primitive we do not have to stack the inputs of each expert along a new dimension (leading to a tensor of size \((N_{experts},capacity, d)\) as shown previously) but we can simply have a 2d input x of size (N_{tokens},d) that is permuted such that the first rows are mapped to expert 1 and so on… On the other hand, the expert matrix \(W_E\) is of binary size \((N_{experts}d_{ff},d)\) and the global output matrix (after passing through the 1st of the 2 FFN layers) \(out=x \cdot W_E^T\) will be of size \((N_{tokens},N_{experts}d_{ff})\).</p> <p>We however do not care about most of the elements in the output matrix as for a given token we only want the result of the experts it is matched to. To make it more clear, the dense matrix described above will have the columns \(1,...,d_{ff}\) corresponding to the computation of the 1st expert, the columns \(d_{ff}+1, ...,2d_{ff}\) corresponding to the 2nd expert and so on.</p> <p>This is the exact framing of an SDD (Sparse Dense Dense) block sparse matrix multiplication where we multiply 2 dense matrices to yield a sparse matrix. This sparsity constraint needs to be given through a binary mask that will equal to 1 for blocks we want to compute and 0 otherwise. More precisely, the binary mask corresponding to the position (i,j) will be equal to 1 iff the i-th token is gated to the expert associated to the j-th column.</p> <p>The overall procedure is to tile the matrix into blocks and perform the block multiplication iff one of the values of the mask is 1 for this block. After the computation, we mask again the output as a block containing 0 and 1s will be computed and should have some of these values masked.</p> <h4 id="why-do-we-need-to-have-block-sparsity-and-not-sparsity">Why do we need to have block sparsity and not sparsity?</h4> <p>Computing a masked output introduces at some point or another a conditional statement as we want to perform the computation only if the value of the mask is 1. As conditional statements are way slower on GPUs than arithmetic operations, we want to perform as much operations per if statement as possible by increasing the block size to achieve peak performance and arithmetic intensity. On the other extreme, if the block size is the size of the matrix, the global operation consists in performing the very inefficient full dense matrix multiplication to then apply the mask. The block size should then be tuned with care to ensure peak performance.</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mixture_of_experts/sparse_vs_block_sparse-480.webp 480w,/assets/img/mixture_of_experts/sparse_vs_block_sparse-800.webp 800w,/assets/img/mixture_of_experts/sparse_vs_block_sparse-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/mixture_of_experts/sparse_vs_block_sparse.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Illustration of sparse vs block sparse matrices. (Image source: <a href="https://andrew.gibiansky.com/wavernn-demystified-sparsity/" rel="external nofollow noopener" target="_blank">Andrew Gibiansky website</a>) </div> <h4 id="why-do-we-still-need-to-permute-and-unpermute-the-tokens">Why do we still need to permute and unpermute the tokens?</h4> <p>This block sparse matrix multiplication would theoretically work without permuting the tokens. However, this would remove all the structure in the output mask as we would not have anymore all the first tokens mapped to the 1st expert, the following mapped to the 2nd expert. This would thus lead to a sparse matrix that isn’t block sparse at all affecting very negatively performances.</p> <h2 id="distributed-moes">Distributed MoEs</h2> <p>Having large batch sizes is important for compute efficiency during training as they enable to perform more computation each time the matrix weights are moved from the slow HBM (High Bandwidth Memory) to each of the Streaming Multiprocessor (SM) shared memory, thus increasing the arithmetic intensity.</p> <p>However, MoE reduces the effective batch size of each expert by a factor of \(\frac{K}{N_{experts}}\) leading to suboptimal computations.</p> <p>We will see in this section how this problem can be treated when distributing training on several devices.</p> <h3 id="data-parallelism">Data Parallelism</h3> <p>In the case of MoE, a naive approach to distribute training on several machines is to equally divide each batch into mini-batches where each mini-batch is sent to a different device that stores the totality of the model weights.</p> <p>This approach is very straightforward and introduces very little communication overhead as you only need to transmit information between devices at the very end when collecting the gradients (this collecting operation is in fact a <a href="https://en.wikipedia.org/wiki/Reduction_operator" rel="external nofollow noopener" target="_blank">reduction operation</a> where we sum the gradients transmitted by each device).</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mixture_of_experts/reduce_operation-480.webp 480w,/assets/img/mixture_of_experts/reduce_operation-800.webp 800w,/assets/img/mixture_of_experts/reduce_operation-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/mixture_of_experts/reduce_operation.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Illustration of a reduce operation. </div> <h2 id="model-parallelism">Model Parallelism</h2> <p>As LLMs are now too big to fit on a single machine, Model Parallelism (MP) splits the model weights among \(m\) machines. MP acts like an horizontal cut in the model weights where each matrix is divided into little parts so that at each matrix multiplication the computational load will be balanced between the \(m\) devices. More precisely, each device uses only \(\frac{d}{m}\) of the hidden dimensions to compute its output and the output of each device are then summed together through an all_reduce operation.</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mixture_of_experts/all_reduce-480.webp 480w,/assets/img/mixture_of_experts/all_reduce-800.webp 800w,/assets/img/mixture_of_experts/all_reduce-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/mixture_of_experts/all_reduce.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Illustration of an all_reduce operation. </div> <p>We note that the implementation of the reduce or all_reduce message passing primitives can themselves be optimized based on the topology of the cluster (how each machine is linked to the others), we choose to not dive this deep.</p> <p>Model and Data Parallelism are often used together in what is sometimes called FSDP (Fully Sharded Data Parallelism).</p> <h3 id="expert-parallelism">Expert Parallelism</h3> <p>Expert parallelism simply refers to the idea of storing different subsets of experts on different devices. In this case, the gating operation is in fact really routing the tokens to different machines.</p> <p>Such a notion highlights once again the need for load balancing as an unbalanced gating function would lead to some devices being idle while others are overloaded diminishing the overall efficiency.</p> <h3 id="expert--model--data-parallelism">Expert + Model + Data Parallelism</h3> <p>One can use a clever trick to roughly maintain the same batch size for experts and for the rest of the model. If we perform Fully Sharded Data Parallelism with \(\frac{N_{experts}}{K}\) mini-batches of size \(\mathcal{B}\) and then apply expert parallelism on the outputs of all the mini-batches. As we saw previously, a balanced load leads to each expert receiving \(\frac{K}{N_{experts}}\) of the total examples so \(\frac{K}{N_{experts}}(\frac{N_{experts}}{K}\mathcal{B})=\mathcal{B}\) examples.</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mixture_of_experts/expert_parallelism-480.webp 480w,/assets/img/mixture_of_experts/expert_parallelism-800.webp 800w,/assets/img/mixture_of_experts/expert_parallelism-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/mixture_of_experts/expert_parallelism.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> An illustration of Data Parallelism, Expert Parallelism, and Model Parallelism. Each square corresponds to distinct copies of the weights and each color corresponds to different experts to assign to. (Image source: <a class="citation" href="#switch_transformer">(Fedus, 2022)</a>) </div> <h1 id="references">References</h1> <ol class="bibliography"> <li><span id="mixture_density_network">Bishop. (2024). <i>Mixture Density Networks</i>. https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf</span></li> <li><span id="articulatory_inversion">Uria, et al. (2012). <i>Deep Architectures for Articulatory Inversion</i>. https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf</span></li> <li><span id="mixtral_of_experts">Jiang, et al. (2024). <i>Mixtral Of Experts</i>. https://arxiv.org/pdf/2401.04088</span></li> <li><span id="scaling_laws">Kaplan, et al. (2020). <i>Scaling Laws for Neural Language Models</i>.</span></li> <li><span id="deepseekmoe">Dai, et al. (2024). <i>DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models</i>. https://arxiv.org/pdf/2401.06066</span></li> <li><span id="sparse_moe">Shazeer, et al. (2017). <i>Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</i>. https://arxiv.org/pdf/1701.06538</span></li> <li><span id="gshard">Lepikhin, et al. (2020). <i>GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding</i>. https://arxiv.org/pdf/2006.16668</span></li> <li><span id="loss_free_balancing">Wang, et al. (2024). <i>Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts</i>. https://arxiv.org/pdf/2408.15664</span></li> <li><span id="megablocks">Gale, et al. (2022). <i>MegaBlocks: Efficient Sparse Training with Mixture-of-Experts</i>. https://arxiv.org/pdf/2408.15664</span></li> <li><span id="switch_transformer">Fedus, et al. (2022). <i>Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</i>. https://arxiv.org/pdf/2101.03961</span></li> </ol> </div> </article> <br> <hr> <br> If you found this useful, please cite this as: <blockquote> <p>Martin, Guillaume (Oct 2024). Mixture of Experts: From the ground up to use in LLMs.. https://guillaumemartinfesta.github.io.</p> </blockquote> <p>or as a BibTeX entry:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">martin2024mixture-of-experts-from-the-ground-up-to-use-in-llms</span><span class="p">,</span>
  <span class="na">title</span>   <span class="p">=</span> <span class="s">{Mixture of Experts: From the ground up to use in LLMs.}</span><span class="p">,</span>
  <span class="na">author</span>  <span class="p">=</span> <span class="s">{Martin, Guillaume}</span><span class="p">,</span>
  <span class="na">year</span>    <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span>   <span class="p">=</span> <span class="s">{Oct}</span><span class="p">,</span>
  <span class="na">url</span>     <span class="p">=</span> <span class="s">{https://guillaumemartinfesta.github.io/blog/2024/moe/}</span>
<span class="p">}</span>
</code></pre></div></div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Guillaume Martin. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>